{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 03: Logistic Regression\n",
                "\n",
                "**CS229 Aligned Curriculum** | *Gold Standard Edition*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìñ 0. Definition & When to Use\n",
                "\n",
                "### What is Logistic Regression?\n",
                "\n",
                "Alright, let's start with the basics. **Logistic Regression** is a classification algorithm for **binary outcomes** (0 or 1).\n",
                "\n",
                "**Formula:** $P(y=1|x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$\n",
                "\n",
                "**Output:** A probability between 0 and 1\n",
                "\n",
                "**Decision Rule:** Predict 1 if $P(y=1) \\geq$ threshold (default 0.5)\n",
                "\n",
                "---\n",
                "\n",
                "### üéØ When to Use Logistic Regression?\n",
                "\n",
                "| Scenario | ‚úÖ Use It | ‚ùå Don't Use It |\n",
                "|----------|---------|----------------|\n",
                "| **Binary classification** | ‚úÖ Spam/Not, Pass/Fail, Fraud/Legit | ‚ùå Multi-class (use Softmax) |\n",
                "| **Probability needed** | ‚úÖ Risk scoring, confidence levels | ‚ùå Just hard labels (use Perceptron) |\n",
                "| **Interpretable coefficients** | ‚úÖ Medical diagnosis explanation | ‚ùå Black-box acceptable (use NN) |\n",
                "| **Linear decision boundary OK** | ‚úÖ Linearly separable data | ‚ùå Complex boundaries (use SVM/NN) |\n",
                "\n",
                "---\n",
                "\n",
                "### üåç Real-World Examples:\n",
                "\n",
                "üí≥ **Fraud Detection**: P(Fraud) > 0.8 ‚Üí Block transaction\n",
                "üè• **Medical Diagnosis**: P(Disease) based on symptoms, lab results\n",
                "üìß **Email Filtering**: Spam vs Not Spam classification\n",
                "üéì **Student Admission**: Predict acceptance probability based on GPA, SAT\n",
                "\n",
                "---\n",
                "\n",
                "**üí° Rule of Thumb:**\n",
                "- Linear models (Linear/Logistic/Softmax) = **interpretable, fast, good baseline**\n",
                "- Non-linear models (SVM/Neural Nets) = **higher accuracy, less interpretable**\n",
                "- Always start simple ‚Üí Add complexity if needed!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìñ 1. Introduction: Linear vs Logistic Regression\n",
                "\n",
                "### üéØ When to Use Linear vs Logistic?\n",
                "\n",
                "Here's a quick comparison to help you choose:\n",
                "\n",
                "| Aspect | Linear Regression | Logistic Regression |\n",
                "|-------|-------------------|---------------------|\n",
                "| **Output** | Continuous (numbers) | Binary/Categorical (class) |\n",
                "| **Range** | $(-\\infty, +\\infty)$ | $[0, 1]$ (probability) |\n",
                "| **Example** | Predict house price | Spam vs Not Spam |\n",
                "| **Loss Function** | MSE (Mean Squared Error) | Cross-Entropy |\n",
                "| **Decision** | Direct prediction | Threshold (e.g., > 0.5) |\n",
                "\n",
                "---\n",
                "\n",
                "### üé≠ Beginner's Analogy: Email Spam Filter\n",
                "\n",
                "**Scenario:** You want to filter spam emails.\n",
                "\n",
                "**‚ùå WRONG: Using Linear Regression**\n",
                "```\n",
                "Model output: 0.3, 1.5, -0.2, 0.95\n",
                "Problem: What does 1.5 mean? \"Super spam\"? -0.2 = \"Anti-spam\"?\n",
                "```\n",
                "\n",
                "**‚úÖ CORRECT: Using Logistic Regression**\n",
                "```\n",
                "Model output: 0.3, 0.95, 0.05, 0.87\n",
                "Interpretation: P(Spam) - probability between 0-1!\n",
                "Decision: If P(Spam) > 0.5 ‚Üí Spam ‚úÖ\n",
                "```\n",
                "\n",
                "**üí° Key Insight:** Logistic transforms linear output into probability using the **Sigmoid Function**.\n",
                "\n",
                "---\n",
                "\n",
                "### üåü Level 2: Doctor Diagnosis Analogy\n",
                "\n",
                "Think about it this way. A doctor sees patient symptoms: fever (X‚ÇÅ), cough (X‚ÇÇ), shortness of breath (X‚ÇÉ).\n",
                "\n",
                "**Linear Regression:** Output = 1.2 (what does that mean? 120% sick?)\n",
                "\n",
                "**Logistic Regression:** Output = 0.85 = **85% probability** patient has the disease.\n",
                "\n",
                "**Threshold:**\n",
                "- P > 0.8 ‚Üí Positive diagnosis, prescribe treatment\n",
                "- P < 0.2 ‚Üí Negative diagnosis, patient is healthy\n",
                "- 0.2 < P < 0.8 ‚Üí Gray area, further testing needed\n",
                "\n",
                "**üí° Trade-off:**\n",
                "- **Low threshold (0.3):** Sensitive, detects more sick patients (but high false alarm rate)\n",
                "- **High threshold (0.9):** Specific, only diagnoses when certain (but misses some cases)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìê 1.5 Mathematical Derivation: MLE ‚Üí Cross-Entropy Loss\n",
                "\n",
                "### üéØ Why Cross-Entropy?\n",
                "\n",
                "Alright, math time ‚Äì but don't worry, we'll walk through it step by step.\n",
                "\n",
                "**Step 1: Likelihood for 1 Data Point**\n",
                "\n",
                "For binary classification, we assume:\n",
                "$$P(y|x) = \\begin{cases} h_\\theta(x) & \\text{if } y = 1 \\\\ 1 - h_\\theta(x) & \\text{if } y = 0 \\end{cases}$$\n",
                "\n",
                "We can write this more compactly:\n",
                "$$P(y|x; \\theta) = h_\\theta(x)^y \\cdot (1 - h_\\theta(x))^{1-y}$$\n",
                "\n",
                "**Step 2: Likelihood for All Data**\n",
                "\n",
                "$$\\mathcal{L}(\\theta) = \\prod_{i=1}^{m} h_\\theta(x^{(i)})^{y^{(i)}} \\cdot (1 - h_\\theta(x^{(i)}))^{1-y^{(i)}}$$\n",
                "\n",
                "**Step 3: Log-Likelihood (for convenience)**\n",
                "\n",
                "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^{m} y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1 - h_\\theta(x^{(i)}))$$\n",
                "\n",
                "**Step 4: Maximize ‚Üí Minimize Negative**\n",
                "\n",
                "$$J(\\theta) = -\\ell(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1 - h_\\theta(x^{(i)})) \\right]$$\n",
                "\n",
                "**‚úÖ And there it is ‚Äì CROSS-ENTROPY LOSS!**\n",
                "\n",
                "This is one of those beautiful moments in ML where probability theory and optimization align perfectly.\n",
                "\n",
                "---\n",
                "\n",
                "### üîç Intuition: Why Log?\n",
                "\n",
                "**Analogy: Escalating Penalty**\n",
                "\n",
                "| Prediction | Actual | Loss | Interpretation |\n",
                "|----------|--------|------|--------------|\n",
                "| **0.99** | 1 | 0.01 | ‚úÖ Great, small penalty |\n",
                "| **0.5** | 1 | 0.69 | ‚ö†Ô∏è Uncertain |\n",
                "| **0.01** | 1 | 4.61 | ‚ùå Completely wrong, HUGE penalty |\n",
                "\n",
                "**üí° Key Insight:** Cross-Entropy **heavily penalizes** wrong predictions made with high confidence!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ 2. Statistical Framework: Maximum Likelihood\n",
                "\n",
                "### Why Cross-Entropy Loss?\n",
                "\n",
                "Alright, here's where the stats come in. We model P(y=1|x) = œÉ(Œ∏·µÄx) where œÉ is the sigmoid function.\n",
                "\n",
                "For each data point (x, y), the likelihood is:\n",
                "$$P(y|x) = h^y (1-h)^{1-y}$$\n",
                "\n",
                "Log-likelihood:\n",
                "$$\\ell(\\theta) = \\sum [y \\log h + (1-y) \\log(1-h)]$$\n",
                "\n",
                "**Maximize log-likelihood = Minimize Cross-Entropy Loss!** See how elegant that is?\n",
                "\n",
                "### Why Sigmoid?\n",
                "\n",
                "Now here's what makes sigmoid so clever:\n",
                "\n",
                "1. Output is always between 0 and 1 (valid probability)\n",
                "2. Easy derivative: œÉ'(z) = œÉ(z)(1 - œÉ(z))\n",
                "3. Derives from GLM with Bernoulli distribution\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# 1. SETUP DATA (2 Classes)\n",
                "np.random.seed(0)\n",
                "m = 100\n",
                "# We have 2 features: x1 (Exam Score 1), x2 (Exam Score 2)\n",
                "X = np.random.randn(m, 2) \n",
                "# Label y: 1 if Exam1 + Exam2 > 0, else 0 (Diagonal boundary)\n",
                "y = (X[:, 0] + X[:, 1] > 0).astype(int).reshape(-1, 1)\n",
                "\n",
                "# Bias Trick (x0 = 1)\n",
                "X_b = np.c_[np.ones((m, 1)), X] \n",
                "\n",
                "print(\"Shape Data:\", X_b.shape)\n",
                "print(\"Shape Label:\", y.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. MANUAL FUNCTIONS\n",
                "\n",
                "def sigmoid(z):\n",
                "    # Formula: 1 / (1 + e^-z)\n",
                "    return 1 / (1 + np.exp(-z))\n",
                "\n",
                "def hypothesis(X, theta):\n",
                "    z = X.dot(theta)\n",
                "    return sigmoid(z)\n",
                "\n",
                "def compute_cost(X, y, theta):\n",
                "    m = len(y)\n",
                "    h = hypothesis(X, theta)\n",
                "    # Prevent log(0) error with small epsilon\n",
                "    epsilon = 1e-5 \n",
                "    \n",
                "    # Log Likelihood formula (Negative because we want to minimize cost)\n",
                "    # J = -1/m * sum( y*log(h) + (1-y)*log(1-h) )\n",
                "    term1 = y * np.log(h + epsilon)\n",
                "    term2 = (1 - y) * np.log(1 - h + epsilon)\n",
                "    cost = -(1/m) * np.sum(term1 + term2)\n",
                "    return cost\n",
                "\n",
                "def gradient_descent(X, y, theta, alpha, n_iterations):\n",
                "    m = len(y)\n",
                "    cost_history = []\n",
                "    \n",
                "    for i in range(n_iterations):\n",
                "        # 1. Forward Pass\n",
                "        preds = hypothesis(X, theta)\n",
                "        \n",
                "        # 2. Error Vector (Predictions - Actual)\n",
                "        error = preds - y\n",
                "        \n",
                "        # 3. Gradient Calculation\n",
                "        # Same structure as Linear Regression!\n",
                "        gradients = (1/m) * X.T.dot(error)\n",
                "        \n",
                "        # 4. Update Theta\n",
                "        theta = theta - alpha * gradients\n",
                "        \n",
                "        cost_history.append(compute_cost(X, y, theta))\n",
                "        \n",
                "    return theta, cost_history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. TRAINING\n",
                "theta_init = np.zeros((3, 1)) # Start from zeros\n",
                "alpha = 0.5\n",
                "iterations = 1000\n",
                "\n",
                "theta_final, history = gradient_descent(X_b, y, theta_init, alpha, iterations)\n",
                "\n",
                "print(\"Final Theta:\", theta_final.ravel())\n",
                "print(\"Should be roughly [0, 1, 1] (since the rule is x1+x2 > 0)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. VISUALIZE DECISION BOUNDARY\n",
                "plt.figure(figsize=(10,4))\n",
                "\n",
                "# Cost History\n",
                "plt.subplot(1,2,1)\n",
                "plt.plot(history)\n",
                "plt.title(\"Cost History (Log Loss)\")\n",
                "\n",
                "# Decision Boundary\n",
                "plt.subplot(1,2,2)\n",
                "# Scatter plot (Blue=0, Red=1)\n",
                "plt.scatter(X[y.ravel()==0, 0], X[y.ravel()==0, 1], color='blue', label='Class 0')\n",
                "plt.scatter(X[y.ravel()==1, 0], X[y.ravel()==1, 1], color='red', label='Class 1')\n",
                "\n",
                "# Draw decision boundary: theta0 + theta1*x1 + theta2*x2 = 0\n",
                "# x2 = -(theta0 + theta1*x1) / theta2\n",
                "x1_vals = np.array([-3, 3])\n",
                "x2_vals = -(theta_final[0] + theta_final[1]*x1_vals) / theta_final[2]\n",
                "plt.plot(x1_vals, x2_vals, \"k--\", label=\"Decision Boundary\")\n",
                "plt.legend()\n",
                "plt.title(\"Classification Results\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç How to Read Output & Graphs\n",
                "\n",
                "**Training Output:**\n",
                "- Final Theta [Œ∏‚ÇÄ, Œ∏‚ÇÅ, Œ∏‚ÇÇ]: Trained model coefficients\n",
                "- Œ∏‚ÇÄ = bias/intercept\n",
                "- Œ∏‚ÇÅ, Œ∏‚ÇÇ ‚âà 1 because data was generated with rule x‚ÇÅ + x‚ÇÇ > 0\n",
                "\n",
                "**Cost History Graph:**\n",
                "- Y-axis = Log Loss (Cross Entropy)\n",
                "- ‚úÖ **Good:** Curve decreases and stabilizes\n",
                "- ‚ö†Ô∏è **Warning:** If it oscillates ‚Üí learning rate too high\n",
                "\n",
                "**Decision Boundary Graph:**\n",
                "- Dashed line = decision boundary (h(x) = 0.5)\n",
                "- Above line ‚Üí Predict 1 (red)\n",
                "- Below line ‚Üí Predict 0 (blue)\n",
                "\n",
                "**Logistic vs Linear Regression:**\n",
                "| Aspect | Linear | Logistic |\n",
                "|-------|--------|----------|\n",
                "| Output | Any number | 0-1 (probability) |\n",
                "| Loss | MSE | Cross Entropy |\n",
                "| Use case | Regression | Classification |\n",
                "\n",
                "**üí° Key Takeaway:** Logistic = Linear Regression + Sigmoid + Cross Entropy!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# üè≠ Industry Examples: Logistic Regression\n",
                "\n",
                "## 1. üí∞ Finance: Loan Default Prediction\n",
                "- Features: Income, credit score, debt ratio\n",
                "- Output: P(default) ‚Üí Approve if < threshold\n",
                "- Interpretable: Coefficient = log odds ratio\n",
                "\n",
                "## 2. üè• Healthcare: Disease Screening\n",
                "- Features: Lab results, symptoms\n",
                "- Output: P(disease | symptoms)\n",
                "- Threshold tuning based on cost of false negative\n",
                "\n",
                "## 3. üì± Tech: Click-Through Rate\n",
                "- Features: User features, ad features\n",
                "- Output: P(click | user, ad)\n",
                "- Real-time serving for ad bidding\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìä 3. Evaluation Metrics (Beyond Accuracy)\n",
                "\n",
                "Alright, so here's the thing ‚Äì accuracy alone won't cut it! This is especially true with **imbalanced data** (think cancer detection: 99% healthy, 1% sick).\n",
                "\n",
                "Check this out: If we just predict \"everyone is healthy\", accuracy is 99%, but the model COMPLETELY FAILS to detect the disease. Pretty bad, right?\n",
                "\n",
                "That's why we need:\n",
                "1. **Confusion Matrix**: See True Positive, False Positive, etc.\n",
                "2. **Precision & Recall**: Trade-off between \"correct predictions\" and \"catching all cases\".\n",
                "3. **F1-Score**: Harmonic mean of Precision & Recall.\n",
                "4. **ROC & AUC**: How good is the model at distinguishing class 0 from class 1?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def evaluate_classification(y_true, y_pred, y_prob=None, title=\"Model Evaluation\"):\n",
                "    # 1. Classification Report & Confusion Matrix\n",
                "    print(f\"üîπ {title} Report:\")\n",
                "    print(classification_report(y_true, y_pred))\n",
                "    \n",
                "    plt.figure(figsize=(12, 5))\n",
                "    \n",
                "    # Plot Confusion Matrix\n",
                "    plt.subplot(1, 2, 1)\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.ylabel('Actual')\n",
                "    plt.title(f'{title} - Confusion Matrix')\n",
                "    \n",
                "    # 2. ROC Curve (if probs provided)\n",
                "    if y_prob is not None:\n",
                "        plt.subplot(1, 2, 2)\n",
                "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
                "        roc_auc = auc(fpr, tpr)\n",
                "        \n",
                "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
                "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
                "        plt.xlim([0.0, 1.0])\n",
                "        plt.ylim([0.0, 1.05])\n",
                "        plt.xlabel('False Positive Rate')\n",
                "        plt.ylabel('True Positive Rate')\n",
                "        plt.title(f'{title} - ROC Curve')\n",
                "        plt.legend(loc=\"lower right\")\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Example usage (Make sure y_test and y_pred exist from previous cells)\n",
                "# evaluate_classification(y_test, predictions, probabilities, title=\"Logistic Regression (Scratch)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç How to Read Confusion Matrix:\n",
                "\n",
                "```\n",
                "              Predicted\n",
                "           |  0  |  1  |\n",
                "Actual  0  | TN  | FP  |\n",
                "        1  | FN  | TP  |\n",
                "```\n",
                "\n",
                "**Explanation:**\n",
                "- **TN (True Negative):** Correctly predicted 0 ‚úÖ\n",
                "- **TP (True Positive):** Correctly predicted 1 ‚úÖ\n",
                "- **FP (False Positive):** Wrong, predicted 1 but was 0 ‚ùå (Type I Error)\n",
                "- **FN (False Negative):** Wrong, predicted 0 but was 1 ‚ùå (Type II Error)\n",
                "\n",
                "**Watch for:**\n",
                "- **High FP:** Model is too \"anxious\" (lots of false alarms)\n",
                "- **High FN:** Model is too \"complacent\" (missing many cases)\n",
                "\n",
                "**üí° Trade-off:** Lower threshold ‚Üí FP goes up, FN goes down"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üöÄ 4. Production Quality & Feature Engineering\n",
                "\n",
                "Alright, now let's talk about taking our model to production. We need **Modularity** and **Automation** to make this work in the real world.\n",
                "\n",
                "### 4.1 Interaction Features\n",
                "Sometimes relationships between variables aren't independent (for example, the effect of 'Age' might differ based on 'Gender'). When that happens, we create interaction features.\n",
                "\n",
                "### 4.2 Pipeline Automation\n",
                "Instead of doing scaling and encoding manually every single time, we wrap everything in a **Pipeline** to ensure consistency between training and serving. This prevents the classic \"works on my machine\" problem!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "# üöÄ 4. PRODUCTION QUALITY & FEATURE ENGINEERING\n",
                "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
                "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Create sample data if not already available\n",
                "if 'X_train' not in dir() or 'X_test' not in dir():\n",
                "    print(\"‚ö†Ô∏è X_train/X_test not found. Creating sample data...\")\n",
                "    from sklearn.datasets import make_classification\n",
                "    X_sample, y_sample = make_classification(\n",
                "        n_samples=200, \n",
                "        n_features=10, \n",
                "        n_informative=8,\n",
                "        n_redundant=2,\n",
                "        random_state=42\n",
                "    )\n",
                "    X_train, X_test, y_train, y_test = train_test_split(\n",
                "        X_sample, y_sample, test_size=0.3, random_state=42\n",
                "    )\n",
                "    print(f\"‚úÖ Created sample data: {X_train.shape}\")\n",
                "\n",
                "# Scale the features first\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Original features: {X_train.shape[1]}\")\n",
                "\n",
                "# 1. Feature Interaction Demo\n",
                "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
                "X_interact = poly.fit_transform(X_train_scaled)\n",
                "print(f\"Features with Interaction: {X_interact.shape[1]}\")\n",
                "print(f\"Increase: {X_interact.shape[1] - X_train_scaled.shape[1]} new features\")\n",
                "\n",
                "# 2. Scikit-Learn Pipeline Automation\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "full_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('interaction', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
                "    ('logistic', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
                "])\n",
                "\n",
                "# Fit pipeline (uses ORIGINAL X_train, not scaled - pipeline handles it!)\n",
                "full_pipeline.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate\n",
                "train_score = full_pipeline.score(X_train, y_train)\n",
                "test_score = full_pipeline.score(X_test, y_test)\n",
                "\n",
                "print(f\"\\nüìä Pipeline Performance:\")\n",
                "print(f\"Train Score: {train_score:.4f}\")\n",
                "print(f\"Test Score: {test_score:.4f}\")\n",
                "\n",
                "if train_score - test_score > 0.1:\n",
                "    print(\"‚ö†Ô∏è Warning: Possible overfitting (train >> test)\")\n",
                "else:\n",
                "    print(\"‚úÖ Good generalization\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç How to Read ROC-AUC:\n",
                "\n",
                "**ROC Curve:** Plots TPR (Recall) vs FPR at various thresholds.\n",
                "\n",
                "**AUC (Area Under Curve):** Measures classifier quality.\n",
                "\n",
                "| AUC Value | Quality | Interpretation |\n",
                "|-----------|----------|---------------|\n",
                "| **1.0** | Perfect | Flawless model |\n",
                "| **0.9 - 1.0** | Excellent | Production-ready |\n",
                "| **0.8 - 0.9** | Good | Acceptable |\n",
                "| **0.7 - 0.8** | Fair | Needs improvement |\n",
                "| **0.5 - 0.7** | Poor | Bad model |\n",
                "| **0.5** | Random | No better than coin flip! |\n",
                "\n",
                "**If ROC curve approaches top-left:** Good classifier (high TPR with low FPR)\n",
                "\n",
                "**If ROC curve = diagonal:** Random guess, model failed!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üé¨ 5. Animated Visualizations\n",
                "\n",
                "Now here's the fun part ‚Äì let's see gradient descent in action!\n",
                "\n",
                "### 5.1 Decision Boundary Evolution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.animation import FuncAnimation\n",
                "from IPython.display import HTML\n",
                "\n",
                "np.random.seed(42)\n",
                "\n",
                "# Generate 2-class data\n",
                "X0 = np.random.randn(50, 2) + np.array([-2, -2])\n",
                "X1 = np.random.randn(50, 2) + np.array([2, 2])\n",
                "X = np.vstack([X0, X1])\n",
                "y = np.hstack([np.zeros(50), np.ones(50)])\n",
                "\n",
                "# Add bias term\n",
                "X_b = np.c_[np.ones(100), X]\n",
                "\n",
                "def sigmoid(z):\n",
                "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
                "\n",
                "# Track gradient descent\n",
                "theta = np.zeros(3)\n",
                "theta_history = [theta.copy()]\n",
                "alpha = 0.1\n",
                "\n",
                "for _ in range(50):\n",
                "    h = sigmoid(X_b @ theta)\n",
                "    gradient = X_b.T @ (h - y) / len(y)\n",
                "    theta = theta - alpha * gradient\n",
                "    theta_history.append(theta.copy())\n",
                "\n",
                "# Animation\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "xx = np.linspace(-5, 5, 100)\n",
                "\n",
                "def animate(frame):\n",
                "    ax.clear()\n",
                "    theta = theta_history[min(frame, len(theta_history)-1)]\n",
                "    \n",
                "    ax.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=50, label='Class 0', alpha=0.7)\n",
                "    ax.scatter(X[y==1, 0], X[y==1, 1], c='red', s=50, label='Class 1', alpha=0.7)\n",
                "    \n",
                "    if abs(theta[2]) > 0.001:\n",
                "        yy = -(theta[0] + theta[1] * xx) / theta[2]\n",
                "        ax.plot(xx, yy, 'g-', lw=3, label='Decision Boundary')\n",
                "        ax.fill_between(xx, yy, 5, alpha=0.1, color='red')\n",
                "        ax.fill_between(xx, -5, yy, alpha=0.1, color='blue')\n",
                "    \n",
                "    ax.set_xlim(-5, 5)\n",
                "    ax.set_ylim(-5, 5)\n",
                "    ax.set_xlabel('Feature 1')\n",
                "    ax.set_ylabel('Feature 2')\n",
                "    ax.set_title(f'Logistic Regression - Iteration {frame}')\n",
                "    ax.legend(loc='upper left')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "    return []\n",
                "\n",
                "anim = FuncAnimation(fig, animate, frames=len(theta_history), interval=200, blit=True)\n",
                "plt.close()\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù 9. Exercises with Solutions\n",
                "\n",
                "Let's test your understanding with some practice problems!\n",
                "\n",
                "### Exercise 1: Sigmoid Properties\n",
                "\n",
                "**Q:** Prove that œÉ(-z) = 1 - œÉ(z)\n",
                "\n",
                "<details>\n",
                "<summary>üîë Solution</summary>\n",
                "\n",
                "$$\\sigma(-z) = \\frac{1}{1+e^{z}} = \\frac{1}{1+e^z} \\cdot \\frac{e^{-z}}{e^{-z}} = \\frac{e^{-z}}{e^{-z}+1}$$\n",
                "\n",
                "$$1 - \\sigma(z) = 1 - \\frac{1}{1+e^{-z}} = \\frac{1+e^{-z}-1}{1+e^{-z}} = \\frac{e^{-z}}{1+e^{-z}}$$\n",
                "\n",
                "They're the same! ‚úÖ\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "### Exercise 2: Threshold Selection\n",
                "\n",
                "**Q:** For medical screening, is a threshold of 0.5 good? Why or why not?\n",
                "\n",
                "<details>\n",
                "<summary>üîë Solution</summary>\n",
                "\n",
                "No! Cost of FN (missing disease) >> Cost of FP (extra test).\n",
                "\n",
                "Use a lower threshold (e.g., 0.2) to:\n",
                "- Achieve High Recall (catch most positives)\n",
                "- Accept lower Precision (more false positives)\n",
                "\n",
                "Better safe than sorry!\n",
                "</details>\n",
                "\n",
                "---\n",
                "\n",
                "### Exercise 3: Coefficient Interpretation\n",
                "\n",
                "**Q:** If Œ∏_income = 0.05, what does it mean?\n",
                "\n",
                "<details>\n",
                "<summary>üîë Solution</summary>\n",
                "\n",
                "For every ‚Üë1 unit increase in income:\n",
                "- Log-odds ‚Üë by 0.05\n",
                "- Odds ratio = e^0.05 ‚âà 1.05\n",
                "- Probability of positive class ‚Üë ~5% (approximately, depends on baseline)\n",
                "\n",
                "Note: Odds ratio interpretation is more accurate than probability!\n",
                "</details>\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üöÄ 10. Deployment Case Study: Fraud Detection API\n",
                "\n",
                "Now let's see how this works in the real world!\n",
                "\n",
                "### Architecture\n",
                "```\n",
                "Transaction ‚Üí Feature Extraction ‚Üí Logistic Model ‚Üí Risk Score ‚Üí Decision\n",
                "```\n",
                "\n",
                "### FastAPI Code\n",
                "```python\n",
                "from fastapi import FastAPI\n",
                "import joblib\n",
                "import numpy as np\n",
                "\n",
                "app = FastAPI()\n",
                "model = joblib.load('fraud_detector.pkl')\n",
                "scaler = joblib.load('scaler.pkl')\n",
                "\n",
                "@app.post('/predict_fraud')\n",
                "def predict(amount: float, hour: int, distance_km: float):\n",
                "    X = np.array([[amount, hour, distance_km]])\n",
                "    X_scaled = scaler.transform(X)\n",
                "    prob = model.predict_proba(X_scaled)[0, 1]\n",
                "    \n",
                "    decision = 'BLOCK' if prob > 0.7 else 'REVIEW' if prob > 0.3 else 'APPROVE'\n",
                "    return {'fraud_probability': round(prob, 3), 'decision': decision}\n",
                "```\n",
                "\n",
                "### Monitoring Metrics\n",
                "| Metric | Target | Alert |\n",
                "|--------|--------|-------|\n",
                "| Precision | > 0.8 | < 0.6 |\n",
                "| Recall | > 0.9 | < 0.7 |\n",
                "| Latency (p99) | < 50ms | > 100ms |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "try:\n",
                "    from numba import njit\n",
                "    HAS_NUMBA = True\n",
                "except ImportError:\n",
                "    HAS_NUMBA = False\n",
                "    # Mock njit decorator if not available\n",
                "    def njit(func):\n",
                "        return func\n",
                "\n",
                "@njit\n",
                "def _sigmoid_fast(z):\n",
                "    \"\"\"Sigmoid function with Numba acceleration.\"\"\"\n",
                "    return 1 / (1 + np.exp(-z))\n",
                "\n",
                "class LogisticRegressionScratch:\n",
                "    \"\"\"\n",
                "    Optimized Logistic Regression implementation from scratch.\n",
                "    Supports: Mini-batch Gradient Descent, Early Stopping, and Class Weighting.\n",
                "    \"\"\"\n",
                "    def __init__(self, learning_rate=0.01, n_iterations=1000, batch_size=None, \n",
                "                 tol=1e-4, patience=10, class_weight=None):\n",
                "        self.lr = learning_rate\n",
                "        self.n_iterations = n_iterations\n",
                "        self.batch_size = batch_size # If None, use full batch\n",
                "        self.tol = tol # Tolerance for Early Stopping\n",
                "        self.patience = patience # Wait epochs before stopping\n",
                "        self.class_weight = class_weight # 'balanced' or dict\n",
                "        self.theta = None\n",
                "        self.cost_history = []\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Train the model using Gradient Descent.\n",
                "        Includes shape validation and early stopping logic.\n",
                "        \"\"\"\n",
                "        try:\n",
                "            m, n = X.shape\n",
                "            self.theta = np.zeros(n)\n",
                "            \n",
                "            # Compute weights for handling imbalance\n",
                "            weights = self._get_weights(y, m)\n",
                "            \n",
                "            best_cost = np.inf\n",
                "            wait_count = 0\n",
                "            \n",
                "            batch_size = self.batch_size if self.batch_size else m\n",
                "            \n",
                "            for i in range(self.n_iterations):\n",
                "                # Shuffle for Mini-batch / Stochastic behavior\n",
                "                indices = np.random.permutation(m)\n",
                "                X_shuffled = X[indices]\n",
                "                y_shuffled = y[indices]\n",
                "                w_shuffled = weights[indices]\n",
                "                \n",
                "                for j in range(0, m, batch_size):\n",
                "                    X_batch = X_shuffled[j:j+batch_size]\n",
                "                    y_batch = y_shuffled[j:j+batch_size]\n",
                "                    w_batch = w_shuffled[j:j+batch_size]\n",
                "                    \n",
                "                    # Vectorized forward pass and gradient update\n",
                "                    z = np.dot(X_batch, self.theta)\n",
                "                    h = _sigmoid_fast(z)\n",
                "                    error = h - y_batch\n",
                "                    gradient = (1/len(y_batch)) * np.dot(X_batch.T, error * w_batch)\n",
                "                    self.theta -= self.lr * gradient\n",
                "                \n",
                "                # Calculate cost on full training set for monitoring\n",
                "                full_h = _sigmoid_fast(np.dot(X, self.theta))\n",
                "                current_cost = self._compute_cost(y, full_h, weights, m)\n",
                "                self.cost_history.append(current_cost)\n",
                "                \n",
                "                # Early Stopping Logic\n",
                "                if current_cost < best_cost - self.tol:\n",
                "                    best_cost = current_cost\n",
                "                    wait_count = 0\n",
                "                else:\n",
                "                    wait_count += 1\n",
                "                    if wait_count >= self.patience:\n",
                "                        print(f\"Early stopping triggered at iteration {i}\")\n",
                "                        break\n",
                "                        \n",
                "        except Exception as e:\n",
                "            print(f\"Error during fitting: {e}\")\n",
                "\n",
                "    def _get_weights(self, y, m):\n",
                "        if self.class_weight == 'balanced':\n",
                "            n0 = np.sum(y == 0)\n",
                "            n1 = np.sum(y == 1)\n",
                "            w0 = m / (2 * n0)\n",
                "            w1 = m / (2 * n1)\n",
                "            return np.where(y == 0, w0, w1)\n",
                "        elif isinstance(self.class_weight, dict):\n",
                "            return np.array([self.class_weight[label] for label in y])\n",
                "        return np.ones(m)\n",
                "\n",
                "    def _compute_cost(self, y, h, weights, m):\n",
                "        epsilon = 1e-15\n",
                "        return -(1/m) * np.sum(weights * (y * np.log(h+epsilon) + (1-y) * np.log(1-h+epsilon)))\n",
                "\n",
                "    def predict_proba(self, X):\n",
                "        return _sigmoid_fast(np.dot(X, self.theta))\n",
                "\n",
                "    def predict(self, X, threshold=0.5):\n",
                "        return (self.predict_proba(X) >= threshold).astype(int)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üö¢ 6. Real World Case Study - Titanic Survival\n",
                "\n",
                "Now let's apply Logistic Regression to the legendary **Titanic** dataset!\n",
                "Target: Predict `Survived` (1 = Survived, 0 = Did not survive).\n",
                "\n",
                "Real-World Challenges:\n",
                "- **Categorical** data (Sex, Embarked) ‚Üí Need Encoding.\n",
                "- **Missing** data (Age, Cabin) ‚Üí Need Imputation.\n",
                "- **Imbalanced** data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Load Data\n",
                "import pandas as pd\n",
                "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
                "df = pd.read_csv(url)\n",
                "\n",
                "print(\"Shape:\", df.shape)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Preprocessing & Feature Engineering\n",
                "\n",
                "# Handle Missing Values\n",
                "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
                "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
                "\n",
                "# Select Features\n",
                "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
                "X = df[features].copy()\n",
                "y = df['Survived']\n",
                "\n",
                "# One-Hot Encoding Manual (for understanding)\n",
                "X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n",
                "\n",
                "# Check correlation/imbalance\n",
                "print(\"Class Distribution:\\n\", y.value_counts(normalize=True))\n",
                "\n",
                "# Train Test Split\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Scaling (Required for Gradient Descent!)\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Add Bias Term manually for our scratch model\n",
                "X_train_b = np.c_[np.ones((len(X_train_scaled), 1)), X_train_scaled]\n",
                "X_test_b = np.c_[np.ones((len(X_test_scaled), 1)), X_test_scaled]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train Models (Scratch vs Sklearn)\n",
                "\n",
                "\n",
                "\n",
                "# A. Scratch Model (Balanced Weight)\n",
                "\n",
                "model_scratch = LogisticRegressionScratch(learning_rate=0.1, n_iterations=3000, class_weight='balanced')\n",
                "\n",
                "model_scratch.fit(X_train_b, y_train.values)\n",
                "\n",
                "\n",
                "\n",
                "# B. Sklearn Model\n",
                "\n",
                "clf_sklearn = LogisticRegression(class_weight='balanced', random_state=42)\n",
                "\n",
                "clf_sklearn.fit(X_train_scaled, y_train)\n",
                "\n",
                "\n",
                "\n",
                "# Plot Loss History Scratch\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "\n",
                "plt.plot(model_scratch.cost_history)\n",
                "\n",
                "plt.title('Training Loss (Scratch Model)')\n",
                "\n",
                "plt.xlabel('Epochs')\n",
                "\n",
                "plt.ylabel('Log Loss')\n",
                "\n",
                "plt.show()\n",
                "\n",
                "\n",
                "\n",
                "# Predictions\n",
                "\n",
                "y_pred_scratch = model_scratch.predict(X_test_b)\n",
                "\n",
                "y_prob_scratch = model_scratch.predict_proba(X_test_b)\n",
                "\n",
                "\n",
                "\n",
                "y_pred_sklearn = clf_sklearn.predict(X_test_scaled)\n",
                "\n",
                "y_prob_sklearn = clf_sklearn.predict_proba(X_test_scaled)[:, 1]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Evaluate SKLEARN\n",
                "evaluate_classification(y_test, y_pred_sklearn, y_prob_sklearn, title=\"Titanic - Sklearn\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß 7. Advanced Tuning & Diagnostics\n",
                "\n",
                "Alright, after training comes the real work. Here's what we need to do:\n",
                "1.  **VIF Analysis**: Check if features are highly correlated with each other (Multicollinearity).\n",
                "2.  **Threshold Tuning**: That default 0.5 threshold? Not always optimal, especially with imbalanced data. We'll find the threshold that maximizes F1-Score.\n",
                "3.  **Cross Validation**: Manually validate our scratch model using K-Fold."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "\n",
                "\n",
                "def calculate_vif(X, feature_names):\n",
                "\n",
                "    vif_data = pd.DataFrame()\n",
                "\n",
                "    vif_data[\"Feature\"] = feature_names\n",
                "\n",
                "    vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
                "\n",
                "    return vif_data\n",
                "\n",
                "\n",
                "\n",
                "# 1. Check Multicollinearity\n",
                "\n",
                "print(\"Checking VIF (Variance Inflation Factor)...\")\n",
                "\n",
                "vif_df = calculate_vif(X_train_scaled, features)\n",
                "\n",
                "print(vif_df)\n",
                "\n",
                "\n",
                "\n",
                "# 2. Threshold Tuning (Optimize F1-Score)\n",
                "\n",
                "thresholds = np.arange(0.1, 1.0, 0.05)\n",
                "\n",
                "f1_scores = []\n",
                "\n",
                "for t in thresholds:\n",
                "\n",
                "    y_p = model_scratch.predict(X_test_b, threshold=t)\n",
                "\n",
                "    f1_scores.append(f1_score(y_test, y_p))\n",
                "\n",
                "\n",
                "\n",
                "best_thresh = thresholds[np.argmax(f1_scores)]\n",
                "\n",
                "print(f\"\\nOptimal Threshold for F1-Score: {best_thresh:.2f}\")\n",
                "\n",
                "\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "\n",
                "plt.plot(thresholds, f1_scores, marker='o')\n",
                "\n",
                "plt.axvline(best_thresh, color='r', linestyle='--', label=f'Best: {best_thresh:.2f}')\n",
                "\n",
                "plt.title('Threshold vs F1-Score')\n",
                "\n",
                "plt.xlabel('Threshold')\n",
                "\n",
                "plt.ylabel('F1 Score')\n",
                "\n",
                "plt.legend()\n",
                "\n",
                "plt.show()\n",
                "\n",
                "\n",
                "\n",
                "# 3. Manual Cross Validation (Bonus)\n",
                "\n",
                "print(\"\\nRunning Manual 5-Fold CV for Scratch Model...\")\n",
                "\n",
                "indices = np.arange(len(X_train_b))\n",
                "\n",
                "fold_size = len(X_train_b) // 5\n",
                "\n",
                "scores = []\n",
                "\n",
                "\n",
                "\n",
                "for i in range(5):\n",
                "\n",
                "    val_idx = indices[i*fold_size : (i+1)*fold_size]\n",
                "\n",
                "    train_idx = np.concatenate([indices[:i*fold_size], indices[(i+1)*fold_size:]])\n",
                "\n",
                "    \n",
                "\n",
                "    X_tr_cv, y_tr_cv = X_train_b[train_idx], y_train.values[train_idx]\n",
                "\n",
                "    X_val_cv, y_val_cv = X_train_b[val_idx], y_train.values[val_idx]\n",
                "\n",
                "    \n",
                "\n",
                "    m_cv = LogisticRegressionScratch(learning_rate=0.1, n_iterations=1000)\n",
                "\n",
                "    m_cv.fit(X_tr_cv, y_tr_cv)\n",
                "\n",
                "    \n",
                "\n",
                "    # Simple Accuracy\n",
                "\n",
                "    acc = np.mean(m_cv.predict(X_val_cv) == y_val_cv)\n",
                "\n",
                "    scores.append(acc)\n",
                "\n",
                "\n",
                "\n",
                "print(f\"CV Scores: {scores}\")\n",
                "\n",
                "print(f\"Average Accuracy: {np.mean(scores):.4f}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç How to Read Precision, Recall, F1:\n",
                "\n",
                "**Precision** = $\\frac{TP}{TP + FP}$ = \"Of those predicted positive, how many were actually correct?\"\n",
                "\n",
                "**Recall** = $\\frac{TP}{TP + FN}$ = \"Of those actually positive, how many were detected?\"\n",
                "\n",
                "**F1-Score** = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$ = Harmonic mean\n",
                "\n",
                "| Metric | Low | High | When to Prioritize |\n",
                "|--------|--------|--------|------------------|\n",
                "| **Precision** | Many false alarms | Few false alarms | Spam detection (false alarms are annoying) |\n",
                "| **Recall** | Missing many cases | Detecting all cases | Cancer screening (don't miss any!) |\n",
                "| **F1** | Imbalanced | Balanced | General use case |\n",
                "\n",
                "**If Precision is high, Recall is low:** Model is conservative (only predicts positive when confident)\n",
                "\n",
                "**If Recall is high, Precision is low:** Model is aggressive (detects many but makes many mistakes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üõ°Ô∏è 8. Regularization (L2 / Ridge)\n",
                "\n",
                "Now here's what happens when our model is **Overfitting** (memorizing training data but failing on new data) ‚Äì we need **Regularization**.\n",
                "\n",
                "The idea is simple: we add a \"penalty\" to the cost function if $\\theta$ values get too large.\n",
                "\n",
                "$$ J(\\theta) = -\\frac{1}{m}\\sum [...] + \\frac{\\lambda}{2m} \\sum \\theta_j^2 $$\n",
                "\n",
                "In Sklearn, this is controlled by parameter `C` (Inverse Regularization Strength):\n",
                "- Small `C` ‚Üí STRONG Regularization ($\\lambda$ large) ‚Üí Simple model (might underfit)\n",
                "- Large `C` ‚Üí WEAK Regularization ($\\lambda$ small) ‚Üí Complex model (might overfit)\n",
                "\n",
                "Let's see the effect of `C` on the decision boundary with some visualization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "# Visualize Effect of C (Conceptual Code)\n",
                "C_values = [0.001, 1, 100]\n",
                "\n",
                "print(\"Experimenting with different C values...\")\n",
                "for c in C_values:\n",
                "    model = LogisticRegression(C=c, max_iter=1000)\n",
                "    model.fit(X_train_scaled, y_train)\n",
                "    acc = model.score(X_test_scaled, y_test)\n",
                "    print(f\"C={c}: Test Accuracy = {acc:.4f}\")\n",
                "    # Smaller C typically means coefficients shrink toward zero"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéì 11. CAPSTONE PROJECT: Customer Churn Predictor\n",
                "\n",
                "### Objective\n",
                "Build an end-to-end Logistic Regression model to predict customer churn.\n",
                "\n",
                "### Requirements (100 pts)\n",
                "\n",
                "#### Part 1: EDA (20 pts)\n",
                "- [ ] Load Telco Churn dataset\n",
                "- [ ] Class distribution analysis\n",
                "- [ ] Feature correlation\n",
                "\n",
                "#### Part 2: Model (30 pts)\n",
                "- [ ] Implement from scratch\n",
                "- [ ] Compare with sklearn\n",
                "- [ ] Regularization tuning\n",
                "\n",
                "#### Part 3: Evaluation (25 pts)\n",
                "- [ ] Confusion matrix\n",
                "- [ ] ROC curve & AUC\n",
                "- [ ] Threshold optimization\n",
                "- [ ] Coefficient interpretation\n",
                "\n",
                "#### Part 4: Deployment (25 pts)\n",
                "- [ ] FastAPI endpoint\n",
                "- [ ] Docker container\n",
                "- [ ] Documentation\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üéì CAPSTONE START CODE\n",
                "# Dataset: Telco Customer Churn\n",
                "# Task: Predict who will Churn (switch to competitor)\n",
                "\n",
                "url_churn = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
                "# df_churn = pd.read_csv(url_churn)\n",
                "# df_churn.head()\n",
                "\n",
                "# 1. EDA & Cleaning (Handle 'TotalCharges' which is string, missing values)\n",
                "# 2. Encoding (Gender, InternetService, etc)\n",
                "# 3. Train Model (Logistic Regression)\n",
                "# 4. Evaluation (Confusion Matrix, Recall is important here!)\n",
                "#    (Why Recall? Because we want to catch ALL potential churners)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

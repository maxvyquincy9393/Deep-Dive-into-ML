# ðŸ“Š Evaluation Metrics Quick Reference

## For Classification Models

---

## ðŸŽ¯ Confusion Matrix Breakdown

```
                    Predicted
                   Neg    Pos
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Actual Neg  â”‚   TN    â”‚   FP    â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Actual Pos  â”‚   FN    â”‚   TP    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Understanding Each Quadrant

| Cell | Name | Meaning | Example (Disease Detection) |
|------|------|---------|----------------------------|
| **TP** | True Positive | Correctly predicted positive | Detected disease when present âœ… |
| **TN** | True Negative | Correctly predicted negative | Cleared healthy patient âœ… |
| **FP** | False Positive | Wrongly predicted positive | False alarm (Type I error) âš ï¸ |
| **FN** | False Negative | Wrongly predicted negative | Missed disease (Type II error) ðŸš¨ |

---

## ðŸ“ Core Metrics Formulas

### Accuracy
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
**Meaning**: Overall correct predictions
**Use when**: Classes are balanced
**Don't use when**: Imbalanced data (99% negative â†’ always predict negative = 99% accuracy!)

### Precision
```
Precision = TP / (TP + FP)
```
**Meaning**: "Of all positive predictions, how many were correct?"
**Use when**: False positives are costly (spam filter - don't block good emails)

### Recall (Sensitivity, TPR)
```
Recall = TP / (TP + FN)
```
**Meaning**: "Of all actual positives, how many did we catch?"
**Use when**: False negatives are costly (disease screening - don't miss patients)

### F1 Score
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
**Meaning**: Harmonic mean of precision and recall
**Use when**: You need balance between precision and recall

### F-beta Score
```
FÎ² = (1 + Î²Â²) Ã— (Precision Ã— Recall) / (Î²Â² Ã— Precision + Recall)
```
- **F0.5**: Weights precision higher (2Ã— more important than recall)
- **F1**: Equal weight
- **F2**: Weights recall higher (2Ã— more important than precision)

---

## ðŸ“ˆ ROC and AUC

### ROC Curve
Plots **True Positive Rate** vs **False Positive Rate** at various thresholds.

```
TPR (Recall) = TP / (TP + FN)
FPR = FP / (FP + TN)
```

### Interpreting ROC

| Curve Position | Quality |
|---------------|---------|
| Top-left corner (0, 1) | Perfect classifier |
| Close to top-left | Excellent model |
| Diagonal line | Random guess (useless) |
| Below diagonal | Worse than random |

### AUC (Area Under Curve)

| AUC Value | Interpretation |
|-----------|----------------|
| 1.0 | Perfect |
| 0.9 - 1.0 | Excellent |
| 0.8 - 0.9 | Good |
| 0.7 - 0.8 | Fair |
| 0.6 - 0.7 | Poor |
| 0.5 | Random |

**ðŸ’¡ Key Insight**: AUC = probability that model ranks random positive higher than random negative.

---

## âš–ï¸ Precision-Recall Trade-off

```
        High Precision              High Recall
             â†“                          â†“
    Few false alarms            Catch all positives
    But miss some cases         But more false alarms
```

### Precision-Recall Curve
Better for **imbalanced datasets** than ROC!

```
Average Precision (AP) = Area under PR curve
```

---

## ðŸŽšï¸ Threshold Selection Guide

| Domain | Priority | Recommended Threshold | Reasoning |
|--------|----------|----------------------|-----------|
| Medical Screening | Recall | **0.2 - 0.4** | Don't miss diseases |
| Spam Detection | Precision | **0.7 - 0.9** | Don't block good emails |
| Fraud Detection | Recall | **0.3 - 0.5** | Catch all fraud |
| Credit Scoring | Balanced | **0.5 - 0.6** | Balance risk |
| Anomaly Detection | Precision | **0.8 - 0.95** | Minimize false alerts |

---

## ðŸŽ¯ Multi-Class Metrics

### Micro Average
Pool all classes together, then calculate metric.
```
Micro-Precision = Total TP / (Total TP + Total FP)
```
**Good when**: All samples matter equally

### Macro Average
Calculate metric for each class, then average.
```
Macro-Precision = (Pâ‚ + Pâ‚‚ + ... + Pâ‚–) / K
```
**Good when**: All classes matter equally

### Weighted Average
Weight by class frequency.
```
Weighted-Precision = Î£ (náµ¢ Ã— Páµ¢) / Î£ náµ¢
```
**Good when**: More common classes should contribute more

---

## ðŸ“Š Quick Decision Tree

```
Which metric to use?
    â”‚
    â”œâ”€â”€ Balanced classes?
    â”‚   â””â”€â”€ YES â†’ Accuracy or F1
    â”‚
    â”œâ”€â”€ Imbalanced classes?
    â”‚   â”œâ”€â”€ FP is costly â†’ Precision
    â”‚   â”œâ”€â”€ FN is costly â†’ Recall
    â”‚   â””â”€â”€ Need balance â†’ F1, F2, or ROC-AUC
    â”‚
    â””â”€â”€ Multi-class?
        â”œâ”€â”€ All samples equal â†’ Micro-Average
        â”œâ”€â”€ All classes equal â†’ Macro-Average
        â””â”€â”€ Weighted by frequency â†’ Weighted-Average
```

---

## ðŸ’» Python Code Snippets

### Classification Report
```python
from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_true, y_pred))
print(confusion_matrix(y_true, y_pred))
```

### ROC-AUC
```python
from sklearn.metrics import roc_curve, roc_auc_score

fpr, tpr, thresholds = roc_curve(y_true, y_prob)
auc = roc_auc_score(y_true, y_prob)

import matplotlib.pyplot as plt
plt.plot(fpr, tpr, label=f'ROC (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
```

### Precision-Recall Curve
```python
from sklearn.metrics import precision_recall_curve, average_precision_score

precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
ap = average_precision_score(y_true, y_prob)

plt.plot(recall, precision, label=f'AP = {ap:.3f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
```

### Optimal Threshold for F1
```python
from sklearn.metrics import f1_score
import numpy as np

thresholds = np.arange(0.1, 1.0, 0.05)
f1_scores = [f1_score(y_true, (y_prob >= t).astype(int)) for t in thresholds]
optimal_threshold = thresholds[np.argmax(f1_scores)]
```

---

## âš ï¸ Common Pitfalls

| Pitfall | Problem | Solution |
|---------|---------|----------|
| Using accuracy on imbalanced data | Misleading results | Use F1, AUC, or PR-AUC |
| Ignoring threshold | Default 0.5 isn't optimal | Tune threshold for domain |
| Not using stratified split | Test set may be imbalanced | Use StratifiedKFold |
| Comparing AUC across datasets | AUC depends on class balance | Use same test set |
| Reporting only one metric | Incomplete picture | Report multiple metrics |

---

## ðŸ“š Key Takeaways

> "Accuracy is the most dangerous metric - it can give you false confidence on imbalanced data."

> "In medicine, 99% precision with 1% recall is USELESS. You need to catch patients!"

> "Always ask: What's the COST of false positives vs false negatives?"

> "If you can only pick one metric, use ROC-AUC for balanced data or PR-AUC for imbalanced."

---

*Created for: Deep Dive into ML - Module 03 Logistic Regression*

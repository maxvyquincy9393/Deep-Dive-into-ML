# ðŸ“‹ Logistic Regression Cheat Sheet

## Quick Reference Guide for Classification

---

## ðŸŽ¯ When to Use Logistic Regression

| âœ… Use It | âŒ Avoid It |
|----------|------------|
| Binary classification (yes/no) | Multi-class (>10 classes) |
| Need probability outputs | Complex non-linear boundaries |
| Interpretable coefficients needed | Images, audio, text |
| Linearly separable data | Small datasets (<50 samples) |
| Fast training required | Highly imbalanced data |

---

## ðŸ“ Core Formulas

### Sigmoid Function (Logistic Function)
```
Ïƒ(z) = 1 / (1 + e^(-z))
```

Maps any real number to [0, 1] - perfect for probability!

### Hypothesis (Prediction)
```
h_Î¸(x) = Ïƒ(Î¸áµ€x) = 1 / (1 + e^(-Î¸áµ€x))
```

Output: P(y=1 | x; Î¸) - probability of positive class

### Decision Rule
```
Å· = 1  if h_Î¸(x) â‰¥ 0.5  (default threshold)
Å· = 0  otherwise
```

### Cost Function (Cross-Entropy / Log Loss)
```
J(Î¸) = -1/m Î£ [yâ½â±â¾ log(h_Î¸(xâ½â±â¾)) + (1-yâ½â±â¾) log(1-h_Î¸(xâ½â±â¾))]
```

Why log? Penalizes confident wrong predictions heavily!

### Gradient (for Gradient Descent)
```
âˆ‚J/âˆ‚Î¸â±¼ = 1/m Î£ (h_Î¸(xâ½â±â¾) - yâ½â±â¾) Â· xâ±¼â½â±â¾
```

Same form as linear regression - elegant!


### Update Rule
```
Î¸â±¼ := Î¸â±¼ - Î± Â· âˆ‚J/âˆ‚Î¸â±¼
```

---

## ðŸ“Š Evaluation Metrics

### Confusion Matrix
```
                    Predicted
                    0       1
Actual  0          TN      FP
        1          FN      TP
```

### Key Metrics

| Metric | Formula | Use When |
|--------|---------|----------|
| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Balanced classes |
| **Precision** | TP/(TP+FP) | Minimize false positives |
| **Recall** | TP/(TP+FN) | Minimize false negatives |
| **F1 Score** | 2Â·(PÂ·R)/(P+R) | Balance P and R |
| **ROC-AUC** | Area under ROC curve | Overall performance |

### When to Prioritize What

| Domain | Priority | Reason |
|--------|----------|--------|
| Medical diagnosis | **Recall** | Don't miss sick patients |
| Spam detection | **Precision** | Don't block good emails |
| Fraud detection | **Recall** | Catch all fraudsters |
| Customer churn | **F1 Score** | Balance matters |

---

## ðŸ”§ Regularization

### L2 Regularization (Ridge)
```
J(Î¸) = -1/m Î£ [cost] + Î»/(2m) Î£ Î¸â±¼Â²
```
- Shrinks coefficients toward zero
- Never exactly zero
- Good when all features matter

### L1 Regularization (Lasso)
```
J(Î¸) = -1/m Î£ [cost] + Î»/m Î£ |Î¸â±¼|
```
- Can zero out coefficients
- Automatic feature selection
- Good for sparse models

### Elastic Net (L1 + L2)
```
J(Î¸) = -1/m Î£ [cost] + Î»â‚ Î£ |Î¸â±¼| + Î»â‚‚ Î£ Î¸â±¼Â²
```
- Best of both worlds

---

## ðŸŽšï¸ Threshold Tuning

Default threshold = 0.5 is NOT always optimal!

### How to Choose Threshold

```python
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_true, y_prob)

# Find optimal F1 threshold
f1_scores = 2 * (precision * recall) / (precision + recall)
optimal_threshold = thresholds[np.argmax(f1_scores)]
```

### Domain-Specific Thresholds

| Domain | Recommended Threshold | Reasoning |
|--------|----------------------|-----------|
| Medical screening | **0.3-0.4** | Lower â†’ catch more cases |
| Spam filter | **0.7-0.8** | Higher â†’ fewer false positives |
| Fraud detection | **0.2-0.3** | Lower â†’ catch more fraud |
| Credit scoring | **0.5-0.6** | Balanced decision |

---

## ðŸ’» Quick Code Snippets

### From Scratch Implementation
```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def predict_proba(X, theta):
    return sigmoid(X @ theta)

def cross_entropy_loss(y, h):
    return -np.mean(y * np.log(h + 1e-15) + (1-y) * np.log(1-h + 1e-15))

def gradient(X, y, h):
    return X.T @ (h - y) / len(y)
```

### sklearn Implementation
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# Train
model = LogisticRegression(C=1.0, penalty='l2', max_iter=1000)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Evaluate
print(classification_report(y_test, y_pred))
print(f"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}")
```

### Complete Pipeline
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(C=0.1))
])

# Cross-validation
cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='roc_auc')
print(f"CV ROC-AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# Train final model
pipeline.fit(X_train, y_train)
```

---

## âš ï¸ Common Pitfalls

| Pitfall | Solution |
|---------|----------|
| Not scaling features | Always StandardScaler before Logistic Reg |
| Using accuracy on imbalanced data | Use F1, ROC-AUC, or Precision/Recall |
| Default threshold = 0.5 | Tune threshold based on domain |
| Ignoring convergence warnings | Increase max_iter or reduce C |
| Feature correlation | Check VIF, consider regularization |

---

## ðŸ”„ Multi-Class Extensions

### One-vs-Rest (OvR)
- Train K binary classifiers
- Each: "class k vs all others"
- Prediction: class with highest probability

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(multi_class='ovr')
```

### Softmax (Multinomial)
- Single model, K outputs
- Uses softmax instead of sigmoid
- More principled for multi-class

```python
model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
```

---

## ðŸ“ˆ Interpreting Coefficients

### Coefficient Meaning
```
log(p/(1-p)) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ...
```

- Î¸â±¼ > 0: Feature increases probability of class 1
- Î¸â±¼ < 0: Feature decreases probability of class 1
- |Î¸â±¼| large: Feature has strong influence

### Odds Ratio
```
For 1-unit increase in xâ±¼:
Odds multiply by e^Î¸â±¼
```

Example: Î¸_age = 0.05 â†’ each year adds 5% to odds

---

## ðŸ§ª Model Diagnostics

### Check These Before Deploying

1. **Learning Curve**: Training vs validation error across data sizes
2. **Calibration Curve**: Predicted probability vs actual frequency
3. **Feature Importance**: |coefficient| bar chart
4. **Residual Analysis**: Deviance residuals
5. **ROC Curve**: Trade-off visualization

### Good Model Indicators

- âœ… ROC-AUC > 0.7 (good), > 0.8 (excellent)
- âœ… Calibration curve close to diagonal
- âœ… Small gap between train and validation error
- âœ… Consistent F1 across cross-validation folds

---

## ðŸ“š Key Insights

> "Logistic regression is the go-to baseline for classification. It's simple, interpretable, and often surprisingly competitive with complex models."

> "The sigmoid function is the key insight - it squashes any input to [0,1], giving us a probability interpretation."

> "Cross-entropy loss is derived from Maximum Likelihood Estimation. It's not arbitrary - it's statistically principled."

> "In medicine, a model with 80% accuracy but 50% recall for disease is DANGEROUS. Always tune for domain-appropriate metrics."

---

## ðŸš€ Production Checklist

- [ ] Features are scaled (StandardScaler)
- [ ] Cross-validation performed (5-10 folds)
- [ ] Threshold tuned for domain
- [ ] Class imbalance addressed (if any)
- [ ] Calibration checked (for probability outputs)
- [ ] Feature importance reviewed
- [ ] Model saved with joblib
- [ ] Monitoring strategy defined

---

*Created for: Deep Dive into ML - Module 03 Logistic Regression*

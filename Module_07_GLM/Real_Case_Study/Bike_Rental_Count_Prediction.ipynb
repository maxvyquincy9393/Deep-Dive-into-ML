{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For nice plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"üö¥ Bike Rental Prediction with Poisson GLM\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144c27b",
   "metadata": {},
   "source": [
    "## üìä Creating Realistic Bike Rental Data\n",
    "\n",
    "---\n",
    "\n",
    "Let's simulate realistic bike rental data. Real bike-sharing data has clear patterns:\n",
    "\n",
    "- **Temperature matters**: More rentals on warm days\n",
    "- **Time of day matters**: Rush hours are busier\n",
    "- **Weekends are different**: Leisure vs commute patterns\n",
    "- **Humidity hurts**: Fewer rentals when it's humid\n",
    "\n",
    "We'll generate data that follows these real-world patterns, with rentals following a **Poisson distribution** ‚Äì exactly what GLM is designed for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Generate hourly data for one year\n",
    "n_samples = 8760  # 365 days √ó 24 hours\n",
    "\n",
    "# Time features\n",
    "hour = np.tile(np.arange(24), 365)\n",
    "day_of_year = np.repeat(np.arange(365), 24)\n",
    "\n",
    "# Is it a weekend? (roughly 2/7 of days)\n",
    "day_of_week = day_of_year % 7\n",
    "is_weekend = (day_of_week >= 5).astype(int)\n",
    "\n",
    "# Temperature: seasonal pattern + daily variation + noise\n",
    "# Summer (day 180) is warmest, winter is coldest\n",
    "seasonal_temp = 15 + 12 * np.sin(2 * np.pi * (day_of_year - 80) / 365)\n",
    "daily_temp = 5 * np.sin(2 * np.pi * (hour - 6) / 24)  # Warmest in afternoon\n",
    "temperature = seasonal_temp + daily_temp + np.random.normal(0, 3, n_samples)\n",
    "temperature = np.clip(temperature, -5, 40)  # Realistic bounds\n",
    "\n",
    "# Humidity: anti-correlated with temperature + random\n",
    "humidity = 70 - 0.5 * temperature + np.random.normal(0, 10, n_samples)\n",
    "humidity = np.clip(humidity, 20, 100) / 100  # Normalize to 0-1\n",
    "\n",
    "# Wind speed: random with some correlation to weather\n",
    "windspeed = np.abs(np.random.normal(12, 8, n_samples))\n",
    "windspeed = np.clip(windspeed, 0, 50)\n",
    "\n",
    "# Is it a holiday? (about 3% of days)\n",
    "is_holiday = np.random.binomial(1, 0.03, n_samples)\n",
    "\n",
    "print(\"üìà Feature Statistics:\")\n",
    "print(f\"   Temperature: {temperature.min():.1f}¬∞C to {temperature.max():.1f}¬∞C (mean: {temperature.mean():.1f}¬∞C)\")\n",
    "print(f\"   Humidity: {humidity.min()*100:.1f}% to {humidity.max()*100:.1f}%\")\n",
    "print(f\"   Wind Speed: {windspeed.min():.1f} to {windspeed.max():.1f} km/h\")\n",
    "print(f\"   Weekend samples: {is_weekend.sum()} ({100*is_weekend.mean():.1f}%)\")\n",
    "print(f\"   Holiday samples: {is_holiday.sum()} ({100*is_holiday.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8722bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the magic: generate counts from Poisson distribution!\n",
    "\n",
    "# The TRUE model (what we'll try to learn):\n",
    "# log(Œª) = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑temp + Œ≤‚ÇÇ¬∑humidity + Œ≤‚ÇÉ¬∑weekend + Œ≤‚ÇÑ¬∑hour_pattern\n",
    "\n",
    "# Hour pattern: bimodal for weekdays (rush hours), unimodal for weekends\n",
    "def hour_effect(h, weekend):\n",
    "    if weekend:\n",
    "        # Weekend: peak around noon\n",
    "        return -0.02 * (h - 13)**2\n",
    "    else:\n",
    "        # Weekday: morning rush (8am) and evening rush (6pm)\n",
    "        morning = np.exp(-0.5 * ((h - 8) / 2)**2)\n",
    "        evening = np.exp(-0.5 * ((h - 18) / 2.5)**2)\n",
    "        return 1.5 * (morning + evening)\n",
    "\n",
    "hour_effects = np.array([hour_effect(h, w) for h, w in zip(hour, is_weekend)])\n",
    "\n",
    "# Construct log(Œª) - the linear predictor\n",
    "log_lambda = (\n",
    "    2.5                           # Baseline (intercept)\n",
    "    + 0.04 * temperature          # Warmer = more rentals\n",
    "    - 1.2 * humidity              # Humid = fewer rentals\n",
    "    - 0.02 * windspeed            # Windy = fewer rentals\n",
    "    + 0.3 * is_weekend            # Weekends are busier (leisure)\n",
    "    - 0.5 * is_holiday            # Holidays are slower\n",
    "    + hour_effects                # Time of day pattern\n",
    ")\n",
    "\n",
    "# The Poisson rate (always positive!)\n",
    "lambda_rate = np.exp(log_lambda)\n",
    "\n",
    "# Generate actual counts from Poisson distribution\n",
    "rentals = np.random.poisson(lambda_rate)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'hour': hour,\n",
    "    'temperature': temperature,\n",
    "    'humidity': humidity,\n",
    "    'windspeed': windspeed,\n",
    "    'is_weekend': is_weekend,\n",
    "    'is_holiday': is_holiday,\n",
    "    'rentals': rentals\n",
    "})\n",
    "\n",
    "print(\"\\nüö¥ Bike Rental Dataset Created!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"\\nRental counts summary:\")\n",
    "print(f\"   Min: {rentals.min()}\")\n",
    "print(f\"   Max: {rentals.max()}\")\n",
    "print(f\"   Mean: {rentals.mean():.2f}\")\n",
    "print(f\"   Variance: {rentals.var():.2f}\")\n",
    "print(f\"   Variance/Mean ratio: {rentals.var()/rentals.mean():.2f}\")\n",
    "print(\"\\nüìã First 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204f0b3",
   "metadata": {},
   "source": [
    "## üîç Exploratory Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "Before we build our model, let's understand the data. For count data, we need to check:\n",
    "\n",
    "1. **Distribution of counts**: Is it roughly Poisson-shaped?\n",
    "2. **Mean-Variance relationship**: Poisson assumes Variance = Mean\n",
    "3. **Feature relationships**: Which features affect rentals most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43caf777",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Distribution of rental counts\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(df['rentals'], bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "\n",
    "# Overlay Poisson distribution with same mean\n",
    "mean_rentals = df['rentals'].mean()\n",
    "x_poisson = np.arange(0, df['rentals'].max())\n",
    "y_poisson = stats.poisson.pmf(x_poisson, mean_rentals)\n",
    "ax1.plot(x_poisson, y_poisson, 'r-', linewidth=2, label=f'Poisson(Œª={mean_rentals:.1f})')\n",
    "\n",
    "ax1.set_xlabel('Number of Rentals', fontsize=12)\n",
    "ax1.set_ylabel('Density', fontsize=12)\n",
    "ax1.set_title('Distribution of Bike Rentals', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Mean-Variance relationship by hour\n",
    "ax2 = axes[0, 1]\n",
    "hourly_stats = df.groupby('hour')['rentals'].agg(['mean', 'var'])\n",
    "ax2.scatter(hourly_stats['mean'], hourly_stats['var'], s=100, c='steelblue', edgecolor='white')\n",
    "# Add y=x line (Poisson assumption)\n",
    "max_val = max(hourly_stats['mean'].max(), hourly_stats['var'].max())\n",
    "ax2.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Var = Mean (Poisson)')\n",
    "ax2.set_xlabel('Mean Rentals', fontsize=12)\n",
    "ax2.set_ylabel('Variance of Rentals', fontsize=12)\n",
    "ax2.set_title('Mean vs Variance (by Hour)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Rentals by hour of day\n",
    "ax3 = axes[1, 0]\n",
    "weekday_hourly = df[df['is_weekend'] == 0].groupby('hour')['rentals'].mean()\n",
    "weekend_hourly = df[df['is_weekend'] == 1].groupby('hour')['rentals'].mean()\n",
    "ax3.plot(weekday_hourly.index, weekday_hourly.values, 'o-', linewidth=2, markersize=6, label='Weekday')\n",
    "ax3.plot(weekend_hourly.index, weekend_hourly.values, 's-', linewidth=2, markersize=6, label='Weekend')\n",
    "ax3.set_xlabel('Hour of Day', fontsize=12)\n",
    "ax3.set_ylabel('Average Rentals', fontsize=12)\n",
    "ax3.set_title('Rental Patterns: Weekday vs Weekend', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.set_xticks(range(0, 24, 3))\n",
    "\n",
    "# 4. Temperature effect\n",
    "ax4 = axes[1, 1]\n",
    "temp_bins = pd.cut(df['temperature'], bins=10)\n",
    "temp_means = df.groupby(temp_bins)['rentals'].mean()\n",
    "ax4.bar(range(len(temp_means)), temp_means.values, color='coral', edgecolor='white')\n",
    "ax4.set_xticks(range(len(temp_means)))\n",
    "ax4.set_xticklabels([f'{int(i.left)}-{int(i.right)}' for i in temp_means.index], rotation=45)\n",
    "ax4.set_xlabel('Temperature (¬∞C)', fontsize=12)\n",
    "ax4.set_ylabel('Average Rentals', fontsize=12)\n",
    "ax4.set_title('Temperature Effect on Rentals', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for overdispersion\n",
    "var_mean_ratio = df['rentals'].var() / df['rentals'].mean()\n",
    "print(f\"\\nüìä Overdispersion Check:\")\n",
    "print(f\"   Variance/Mean ratio: {var_mean_ratio:.2f}\")\n",
    "if var_mean_ratio > 1.5:\n",
    "    print(\"   ‚ö†Ô∏è Some overdispersion detected (variance > mean)\")\n",
    "    print(\"   Consider Negative Binomial for better fit, but Poisson is still reasonable.\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Data looks suitable for Poisson regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797f9f9",
   "metadata": {},
   "source": [
    "## üîß Poisson Regression From Scratch\n",
    "\n",
    "---\n",
    "\n",
    "Alright, now for the fun part! Let's build Poisson regression from scratch.\n",
    "\n",
    "### The Math Behind Poisson GLM\n",
    "\n",
    "For Poisson regression:\n",
    "\n",
    "**Link Function** (log link):\n",
    "$$g(\\mu) = \\log(\\mu) = \\theta^T x$$\n",
    "\n",
    "**Mean Function**:\n",
    "$$\\mu = e^{\\theta^T x}$$\n",
    "\n",
    "**Poisson Probability**:\n",
    "$$P(y | \\mu) = \\frac{\\mu^y e^{-\\mu}}{y!}$$\n",
    "\n",
    "**Negative Log-Likelihood** (what we minimize):\n",
    "$$\\text{NLL} = \\sum_{i=1}^{m} \\left[ \\mu^{(i)} - y^{(i)} \\log(\\mu^{(i)}) \\right] + \\text{const}$$\n",
    "\n",
    "Where $\\mu^{(i)} = e^{\\theta^T x^{(i)}}$\n",
    "\n",
    "**Gradient** (for gradient descent):\n",
    "$$\\nabla_{\\theta} \\text{NLL} = \\sum_{i=1}^{m} (\\mu^{(i)} - y^{(i)}) x^{(i)} = X^T (\\mu - y)$$\n",
    "\n",
    "This is beautifully similar to logistic regression! The difference is just in how we compute $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf304f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonRegression:\n",
    "    \"\"\"\n",
    "    Poisson GLM from scratch.\n",
    "    \n",
    "    This is a Generalized Linear Model with:\n",
    "    - Poisson distribution for the response\n",
    "    - Log link function: log(Œº) = Œ∏·µÄx\n",
    "    \n",
    "    Perfect for count data!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, n_epochs=1000, verbose=True):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = n_epochs\n",
    "        self.verbose = verbose\n",
    "        self.theta = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Add column of ones for intercept term.\"\"\"\n",
    "        return np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    def _compute_mu(self, X):\n",
    "        \"\"\"Compute Poisson mean: Œº = exp(Œ∏·µÄx)\"\"\"\n",
    "        linear_pred = X @ self.theta\n",
    "        # Clip to prevent overflow\n",
    "        linear_pred = np.clip(linear_pred, -20, 20)\n",
    "        return np.exp(linear_pred)\n",
    "    \n",
    "    def _negative_log_likelihood(self, X, y):\n",
    "        \"\"\"Compute Poisson negative log-likelihood.\"\"\"\n",
    "        mu = self._compute_mu(X)\n",
    "        # NLL = sum(Œº - y¬∑log(Œº))\n",
    "        # We ignore the log(y!) term since it's constant w.r.t. Œ∏\n",
    "        nll = np.sum(mu - y * np.log(mu + 1e-10))\n",
    "        return nll / len(y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Poisson regression using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array, shape (m, n)\n",
    "            Feature matrix\n",
    "        y : array, shape (m,)\n",
    "            Count data (non-negative integers)\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Add intercept\n",
    "        X_aug = self._add_intercept(X)\n",
    "        m, n = X_aug.shape\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.theta = np.zeros(n)\n",
    "        self.loss_history = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"üîÑ Training Poisson Regression...\")\n",
    "            print(f\"   Samples: {m}, Features: {n-1} (+intercept)\")\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # Compute predicted mean\n",
    "            mu = self._compute_mu(X_aug)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self._negative_log_likelihood(X_aug, y)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute gradient: X.T @ (Œº - y)\n",
    "            gradient = (1/m) * X_aug.T @ (mu - y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            # Progress reporting\n",
    "            if self.verbose and (epoch + 1) % 200 == 0:\n",
    "                print(f\"   Epoch {epoch+1}/{self.epochs}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n‚úÖ Training complete! Final loss: {self.loss_history[-1]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict expected counts (Œº = exp(Œ∏·µÄx)).\"\"\"\n",
    "        X = np.array(X)\n",
    "        X_aug = self._add_intercept(X)\n",
    "        return self._compute_mu(X_aug)\n",
    "    \n",
    "    def predict_counts(self, X):\n",
    "        \"\"\"Predict rounded counts.\"\"\"\n",
    "        return np.round(self.predict(X)).astype(int)\n",
    "    \n",
    "    def get_coefficients(self):\n",
    "        \"\"\"Return intercept and coefficients.\"\"\"\n",
    "        return {\n",
    "            'intercept': self.theta[0],\n",
    "            'coefficients': self.theta[1:]\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ PoissonRegression class defined!\")\n",
    "print(\"\\nKey methods:\")\n",
    "print(\"   .fit(X, y)     - Train the model\")\n",
    "print(\"   .predict(X)    - Predict expected counts (continuous)\")\n",
    "print(\"   .predict_counts(X) - Predict rounded counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64e050",
   "metadata": {},
   "source": [
    "## üöÄ Training Our Model\n",
    "\n",
    "---\n",
    "\n",
    "Let's prepare the data and train our Poisson regression model from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "feature_cols = ['hour', 'temperature', 'humidity', 'windspeed', 'is_weekend', 'is_holiday']\n",
    "X = df[feature_cols].values\n",
    "y = df['rentals'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (important for gradient descent!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training: {len(X_train):,} samples\")\n",
    "print(f\"   Testing:  {len(X_test):,} samples\")\n",
    "print(f\"\\nüìà Target Statistics (Training):\")\n",
    "print(f\"   Mean: {y_train.mean():.2f}\")\n",
    "print(f\"   Std:  {y_train.std():.2f}\")\n",
    "print(f\"   Min:  {y_train.min()}, Max: {y_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our Poisson regression from scratch!\n",
    "model_scratch = PoissonRegression(\n",
    "    learning_rate=0.01,\n",
    "    n_epochs=1000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model_scratch.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_scratch = model_scratch.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mae_scratch = mean_absolute_error(y_test, y_pred_scratch)\n",
    "rmse_scratch = np.sqrt(mean_squared_error(y_test, y_pred_scratch))\n",
    "\n",
    "print(f\"\\nüìä Model Performance (From Scratch):\")\n",
    "print(f\"   MAE:  {mae_scratch:.2f} bikes\")\n",
    "print(f\"   RMSE: {rmse_scratch:.2f} bikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(model_scratch.loss_history, linewidth=2, color='steelblue')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Negative Log-Likelihood', fontsize=12)\n",
    "ax1.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, len(model_scratch.loss_history))\n",
    "\n",
    "# Actual vs Predicted\n",
    "ax2 = axes[1]\n",
    "sample_idx = np.random.choice(len(y_test), 200, replace=False)\n",
    "ax2.scatter(y_test[sample_idx], y_pred_scratch[sample_idx], alpha=0.5, s=30, color='steelblue')\n",
    "max_val = max(y_test.max(), y_pred_scratch.max())\n",
    "ax2.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Actual Rentals', fontsize=12)\n",
    "ax2.set_ylabel('Predicted Rentals', fontsize=12)\n",
    "ax2.set_title('Actual vs Predicted (From Scratch)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The model converges nicely and predictions follow the diagonal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca43df1",
   "metadata": {},
   "source": [
    "## üî¨ Comparison with Sklearn\n",
    "\n",
    "---\n",
    "\n",
    "Let's compare our from-scratch implementation with sklearn's `PoissonRegressor`. This validates that we implemented it correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor as SklearnPoisson\n",
    "\n",
    "# Train sklearn model\n",
    "model_sklearn = SklearnPoisson(alpha=0, max_iter=1000)  # alpha=0 means no regularization\n",
    "model_sklearn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_sklearn = model_sklearn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mae_sklearn = mean_absolute_error(y_test, y_pred_sklearn)\n",
    "rmse_sklearn = np.sqrt(mean_squared_error(y_test, y_pred_sklearn))\n",
    "\n",
    "print(\"üìä Performance Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Metric':<15} {'From Scratch':>15} {'Sklearn':>15}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'MAE':<15} {mae_scratch:>15.2f} {mae_sklearn:>15.2f}\")\n",
    "print(f\"{'RMSE':<15} {rmse_scratch:>15.2f} {rmse_sklearn:>15.2f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare coefficients\n",
    "print(\"\\nüîç Coefficient Comparison:\")\n",
    "print(\"-\"*60)\n",
    "scratch_coefs = model_scratch.get_coefficients()\n",
    "print(f\"{'Feature':<15} {'Scratch':>15} {'Sklearn':>15} {'Diff':>12}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Intercept':<15} {scratch_coefs['intercept']:>15.4f} {model_sklearn.intercept_:>15.4f} {abs(scratch_coefs['intercept'] - model_sklearn.intercept_):>12.4f}\")\n",
    "\n",
    "for i, feat in enumerate(feature_cols):\n",
    "    s_coef = scratch_coefs['coefficients'][i]\n",
    "    sk_coef = model_sklearn.coef_[i]\n",
    "    print(f\"{feat:<15} {s_coef:>15.4f} {sk_coef:>15.4f} {abs(s_coef - sk_coef):>12.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Our implementation matches sklearn closely!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f15e03",
   "metadata": {},
   "source": [
    "## üìà Model Diagnostics\n",
    "\n",
    "---\n",
    "\n",
    "For any GLM, we should check:\n",
    "\n",
    "1. **Residual analysis**: Are residuals well-behaved?\n",
    "2. **Overdispersion**: Is variance > mean? (Poisson assumption violation)\n",
    "3. **Deviance residuals**: More appropriate for Poisson than raw residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute different types of residuals\n",
    "y_pred = y_pred_sklearn  # Use sklearn predictions\n",
    "\n",
    "# Raw residuals\n",
    "raw_residuals = y_test - y_pred\n",
    "\n",
    "# Pearson residuals: (y - Œº) / sqrt(Œº)\n",
    "# For Poisson, variance = Œº, so this standardizes by the expected variance\n",
    "pearson_residuals = (y_test - y_pred) / np.sqrt(y_pred + 1e-10)\n",
    "\n",
    "# Deviance residuals: sign(y - Œº) * sqrt(2 * (y*log(y/Œº) - (y - Œº)))\n",
    "# More appropriate for Poisson\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    term1 = np.where(y_test > 0, y_test * np.log(y_test / (y_pred + 1e-10)), 0)\n",
    "    term2 = y_test - y_pred\n",
    "    deviance_residuals = np.sign(y_test - y_pred) * np.sqrt(2 * np.abs(term1 - term2))\n",
    "    deviance_residuals = np.nan_to_num(deviance_residuals)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Raw residuals vs fitted\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_pred, raw_residuals, alpha=0.3, s=10, color='steelblue')\n",
    "ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Fitted Values', fontsize=12)\n",
    "ax1.set_ylabel('Raw Residuals', fontsize=12)\n",
    "ax1.set_title('Raw Residuals vs Fitted', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Pearson residuals vs fitted\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_pred, pearson_residuals, alpha=0.3, s=10, color='coral')\n",
    "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.axhline(y=2, color='gray', linestyle=':', alpha=0.5)\n",
    "ax2.axhline(y=-2, color='gray', linestyle=':', alpha=0.5)\n",
    "ax2.set_xlabel('Fitted Values', fontsize=12)\n",
    "ax2.set_ylabel('Pearson Residuals', fontsize=12)\n",
    "ax2.set_title('Pearson Residuals vs Fitted', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Distribution of Pearson residuals\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(pearson_residuals, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "x_norm = np.linspace(-4, 4, 100)\n",
    "ax3.plot(x_norm, stats.norm.pdf(x_norm), 'r-', linewidth=2, label='Standard Normal')\n",
    "ax3.set_xlabel('Pearson Residuals', fontsize=12)\n",
    "ax3.set_ylabel('Density', fontsize=12)\n",
    "ax3.set_title('Distribution of Pearson Residuals', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Q-Q plot\n",
    "ax4 = axes[1, 1]\n",
    "stats.probplot(pearson_residuals, dist='norm', plot=ax4)\n",
    "ax4.set_title('Q-Q Plot of Pearson Residuals', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overdispersion test\n",
    "pearson_chi2 = np.sum(pearson_residuals**2)\n",
    "n_test = len(y_test)\n",
    "dispersion = pearson_chi2 / (n_test - len(feature_cols) - 1)\n",
    "\n",
    "print(f\"\\nüìä Overdispersion Check:\")\n",
    "print(f\"   Pearson Chi-squared: {pearson_chi2:.2f}\")\n",
    "print(f\"   Degrees of freedom:  {n_test - len(feature_cols) - 1}\")\n",
    "print(f\"   Dispersion parameter: {dispersion:.2f}\")\n",
    "\n",
    "if dispersion > 1.5:\n",
    "    print(\"   ‚ö†Ô∏è Evidence of overdispersion. Consider Negative Binomial.\")\n",
    "elif dispersion < 0.5:\n",
    "    print(\"   ‚ö†Ô∏è Evidence of underdispersion.\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Dispersion looks reasonable for Poisson.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f332c49",
   "metadata": {},
   "source": [
    "## üéØ Feature Importance: Interpreting Coefficients\n",
    "\n",
    "---\n",
    "\n",
    "Here's the cool part about Poisson regression: **coefficients have a multiplicative interpretation!**\n",
    "\n",
    "Since $\\log(\\mu) = \\theta^T x$, we have $\\mu = e^{\\theta^T x}$.\n",
    "\n",
    "If we increase feature $x_j$ by 1 unit:\n",
    "$$\\mu_{new} = e^{\\theta_0 + ... + \\theta_j(x_j + 1) + ...} = \\mu_{old} \\cdot e^{\\theta_j}$$\n",
    "\n",
    "So $e^{\\theta_j}$ is the **rate ratio** ‚Äì the multiplicative change in expected count!\n",
    "\n",
    "- $e^{\\theta_j} > 1$: Feature increases count\n",
    "- $e^{\\theta_j} < 1$: Feature decreases count\n",
    "- $e^{\\theta_j} = 1$: No effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "coefficients = model_sklearn.coef_\n",
    "rate_ratios = np.exp(coefficients)\n",
    "\n",
    "# Create a nice summary table\n",
    "print(\"üìä Feature Importance (Rate Ratios)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<15} {'Coefficient':>12} {'Rate Ratio':>12} {'Interpretation'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for feat, coef, rr in zip(feature_cols, coefficients, rate_ratios):\n",
    "    if rr > 1:\n",
    "        change = f\"+{(rr-1)*100:.1f}% per 1 SD increase\"\n",
    "    else:\n",
    "        change = f\"{(rr-1)*100:.1f}% per 1 SD increase\"\n",
    "    print(f\"{feat:<15} {coef:>12.4f} {rr:>12.4f} {change}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nIntercept: {model_sklearn.intercept_:.4f}\")\n",
    "print(f\"Baseline rate (exp(intercept)): {np.exp(model_sklearn.intercept_):.2f} rentals/hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff713886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Coefficients\n",
    "ax1 = axes[0]\n",
    "colors = ['green' if c > 0 else 'red' for c in coefficients]\n",
    "bars = ax1.barh(feature_cols, coefficients, color=colors, alpha=0.7, edgecolor='white')\n",
    "ax1.axvline(x=0, color='black', linewidth=1)\n",
    "ax1.set_xlabel('Coefficient (log scale)', fontsize=12)\n",
    "ax1.set_title('Poisson Regression Coefficients', fontsize=14, fontweight='bold')\n",
    "for i, (feat, coef) in enumerate(zip(feature_cols, coefficients)):\n",
    "    ax1.text(coef + 0.02 * np.sign(coef), i, f'{coef:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Rate ratios\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if rr > 1 else 'red' for rr in rate_ratios]\n",
    "ax2.barh(feature_cols, rate_ratios, color=colors, alpha=0.7, edgecolor='white')\n",
    "ax2.axvline(x=1, color='black', linewidth=2, linestyle='--', label='No effect')\n",
    "ax2.set_xlabel('Rate Ratio (multiplicative effect)', fontsize=12)\n",
    "ax2.set_title('Rate Ratios: exp(coefficient)', fontsize=14, fontweight='bold')\n",
    "for i, (feat, rr) in enumerate(zip(feature_cols, rate_ratios)):\n",
    "    ax2.text(rr + 0.02, i, f'{rr:.2f}x', va='center', fontsize=10)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Temperature has the LARGEST positive effect on rentals\")\n",
    "print(\"   ‚Ä¢ Humidity DECREASES rentals (negative coefficient)\")\n",
    "print(\"   ‚Ä¢ Weekend effect is positive (more leisure riding)\")\n",
    "print(\"   ‚Ä¢ Holidays slightly decrease rentals (less commuting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5f99b",
   "metadata": {},
   "source": [
    "## üîÆ Predictions in Action\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how our model predicts for different scenarios. This is where GLM really shines ‚Äì we can interpret predictions directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d58bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some scenarios to predict\n",
    "scenarios = [\n",
    "    {'name': 'Cold weekday morning', 'hour': 8, 'temperature': 5, 'humidity': 0.8, 'windspeed': 20, 'is_weekend': 0, 'is_holiday': 0},\n",
    "    {'name': 'Warm weekday evening', 'hour': 18, 'temperature': 25, 'humidity': 0.4, 'windspeed': 10, 'is_weekend': 0, 'is_holiday': 0},\n",
    "    {'name': 'Perfect weekend noon', 'hour': 12, 'temperature': 22, 'humidity': 0.5, 'windspeed': 5, 'is_weekend': 1, 'is_holiday': 0},\n",
    "    {'name': 'Hot humid afternoon', 'hour': 15, 'temperature': 35, 'humidity': 0.9, 'windspeed': 5, 'is_weekend': 0, 'is_holiday': 0},\n",
    "    {'name': 'Night shift', 'hour': 3, 'temperature': 15, 'humidity': 0.6, 'windspeed': 10, 'is_weekend': 0, 'is_holiday': 0},\n",
    "]\n",
    "\n",
    "print(\"üîÆ Rental Predictions for Different Scenarios\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Prepare features\n",
    "    X_scenario = np.array([[scenario['hour'], scenario['temperature'], \n",
    "                           scenario['humidity'], scenario['windspeed'],\n",
    "                           scenario['is_weekend'], scenario['is_holiday']]])\n",
    "    X_scenario_scaled = scaler.transform(X_scenario)\n",
    "    \n",
    "    # Predict\n",
    "    pred = model_sklearn.predict(X_scenario_scaled)[0]\n",
    "    \n",
    "    print(f\"\\nüìç {scenario['name']}:\")\n",
    "    print(f\"   Conditions: {scenario['temperature']}¬∞C, {scenario['humidity']*100:.0f}% humidity, {scenario['windspeed']} km/h wind\")\n",
    "    print(f\"   Predicted rentals: {pred:.1f} bikes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° Notice how predictions are ALWAYS positive ‚Äì that's the power of log link!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d2d05",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion: When to Use Poisson Regression\n",
    "\n",
    "---\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Count data needs special treatment**: Standard linear regression can predict negative counts, which is nonsense!\n",
    "\n",
    "2. **Poisson regression is perfect for counts**:\n",
    "   - Log link ensures positive predictions\n",
    "   - Models variance = mean (common in count data)\n",
    "   - Coefficients have multiplicative interpretation\n",
    "\n",
    "3. **The GLM framework** connects:\n",
    "   - **Distribution**: Poisson (for counts)\n",
    "   - **Link function**: log (ensures positivity)\n",
    "   - **Linear predictor**: Œ∏·µÄx (same as linear regression!)\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "| Data Type | Distribution | Link | Use When |\n",
    "|-----------|--------------|------|----------|\n",
    "| Continuous | Normal | Identity | Standard regression |\n",
    "| Binary (0/1) | Bernoulli | Logit | Classification |\n",
    "| **Counts** | **Poisson** | **Log** | **Events, rentals, arrivals** |\n",
    "| Counts (overdispersed) | Negative Binomial | Log | Variance > Mean |\n",
    "| Positive continuous | Gamma | Log | Costs, times |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Poisson regression** is the go-to for count data\n",
    "- Check for **overdispersion** (variance > mean) ‚Äì if severe, use Negative Binomial\n",
    "- **Rate ratios** (exp(Œ≤)) give intuitive multiplicative effects\n",
    "- GLM extends regression to many data types ‚Äì same gradient descent framework!\n",
    "\n",
    "üéì **Master this pattern and you can model any data type!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéØ FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìä Model Performance:\")\n",
    "print(f\"   MAE:  {mae_sklearn:.2f} bikes (average error)\")\n",
    "print(f\"   RMSE: {rmse_sklearn:.2f} bikes\")\n",
    "\n",
    "print(\"\\nüîë Key Features (by importance):\")\n",
    "sorted_idx = np.argsort(np.abs(coefficients))[::-1]\n",
    "for i, idx in enumerate(sorted_idx[:4], 1):\n",
    "    print(f\"   {i}. {feature_cols[idx]}: {coefficients[idx]:+.3f} (rate ratio: {rate_ratios[idx]:.2f}x)\")\n",
    "\n",
    "print(\"\\n‚úÖ Poisson GLM successfully models bike rental counts!\")\n",
    "print(\"   ‚Üí Always positive predictions\")\n",
    "print(\"   ‚Üí Interpretable coefficients\")\n",
    "print(\"   ‚Üí Proper handling of count data variance\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Try Negative Binomial if overdispersion is severe\")\n",
    "print(\"   2. Add more features (season, weather category)\")\n",
    "print(\"   3. Consider zero-inflation if many zeros exist\")\n",
    "print(\"   4. Try time-series approaches for temporal patterns\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 04: Softmax Regression\n",
                "\n",
                "**CS229 Aligned Curriculum** | *Gold Standard Edition*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“– 0. Definition & When to Use\n",
                "\n",
                "### What is Softmax Regression (Multinomial Logistic)?\n",
                "\n",
                "Alright, let's kick things off. **Softmax Regression** is the generalization of Logistic Regression for **multi-class classification** (K â‰¥ 3 classes).\n",
                "\n",
                "**Formula:** $P(y=k|x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}$\n",
                "\n",
                "**Output:** K probabilities that sum to 1\n",
                "\n",
                "**Decision Rule:** Predict the class with highest probability\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸ¯ When to Use Softmax Regression (Multinomial Logistic)?\n",
                "\n",
                "| Scenario | âœ… Use It | âŒ Don't Use It |\n",
                "|----------|---------|----------------|\n",
                "| **Multi-class (K â‰¥ 3)** | âœ… Digit recognition (0-9) | âŒ Binary (use Logistic) |\n",
                "| **Mutually exclusive classes** | âœ… Animal type: cat/dog/bird | âŒ Multi-label (use Binary Relevance) |\n",
                "| **Probability distribution needed** | âœ… Ensemble methods, calibration | âŒ Just hard labels |\n",
                "| **Native multi-class support** | âœ… Softmax (efficient) | âŒ One-vs-Rest (less efficient) |\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸŒ Real-World Examples:\n",
                "\n",
                "ğŸ”¢ **Digit Recognition**: MNIST (0-9), OCR systems\n",
                "ğŸ¨ **Image Classification**: Classify artwork style (Baroque, Renaissance, Modern)\n",
                "ğŸ—£ï¸ **Language Detection**: Identify language from text (English, Spanish, Chinese)\n",
                "ğŸ¥ **Disease Classification**: Diagnose disease type from 10+ possibilities\n",
                "\n",
                "---\n",
                "\n",
                "**ğŸ’¡ Rule of Thumb:**\n",
                "- Linear models (Linear/Logistic/Softmax) = **interpretable, fast, good baseline**\n",
                "- Non-linear models (SVM/Neural Nets) = **higher accuracy, less interpretable**\n",
                "- Always start simple â†’ Add complexity if needed!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“– 1. Introduction: Logistic vs Softmax\n",
                "\n",
                "### ğŸ¯ When to Use Logistic vs Softmax?\n",
                "\n",
                "| Aspect | Logistic Regression | Softmax Regression |\n",
                "|-------|---------------------|--------------------|\n",
                "| **Classes** | Binary (2 classes) | Multi-class (K â‰¥ 3) |\n",
                "| **Output** | Single probability P(y=1) | K probabilities [P(y=0), P(y=1), ..., P(y=K-1)] |\n",
                "| **Sum of Probabilities** | P(y=0) + P(y=1) = 1 | Î£ P(y=k) = 1 |\n",
                "| **Function** | Sigmoid: Ïƒ(z) | Softmax: softmax(z) |\n",
                "| **Loss** | Binary Cross-Entropy | Categorical Cross-Entropy |\n",
                "| **Example** | Spam vs Not Spam | Digit recognition (0-9) |\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸ­ Analogy for Beginners: Voting System\n",
                "\n",
                "**Scenario:** Student council election with **3 candidates** (A, B, C)\n",
                "\n",
                "**âŒ Logistic Regression (Binary):**\n",
                "```\n",
                "Output: P(A wins) = 0.7\n",
                "Problem: What about B and C? Can't handle 3+ options!\n",
                "```\n",
                "\n",
                "**âœ… Softmax Regression (Multi-class):**\n",
                "```\n",
                "Output: P(A) = 0.5, P(B) = 0.3, P(C) = 0.2\n",
                "Sum: 0.5 + 0.3 + 0.2 = 1.0 âœ…\n",
                "Decision: Choose A (highest probability)\n",
                "```\n",
                "\n",
                "**ğŸ’¡ Key Insight:** Softmax is a **generalization** of sigmoid for K classes!\n",
                "\n",
                "---\n",
                "\n",
                "### ğŸŒŸ Level 2: Dice Roll Analogy\n",
                "\n",
                "Think of the model as a **biased dice** with K sides.\n",
                "\n",
                "**Softmax output = Probability for each side:**\n",
                "- Side 1: 40% (favorite)\n",
                "- Side 2: 30%\n",
                "- Side 3: 20%\n",
                "- Side 4: 10%\n",
                "\n",
                "**Model training = Learning the dice bias** based on data:\n",
                "- If training data has many class 1 examples â†’ Softmax output P(class 1) â†‘\n",
                "\n",
                "**Temperature parameter = How \"fair\" the dice:**\n",
                "- T=1: Normal (default)\n",
                "- Tâ†’0: Deterministic (winner-takes-all)\n",
                "- Tâ†’âˆ: Fair dice (all probabilities equal)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ§ª 2. Statistical Framework: Categorical Distribution\n",
                "\n",
                "### Softmax Function\n",
                "$$P(y=k|x) = \\frac{\\exp(\\theta_k^T x)}{\\sum_{j=1}^K \\exp(\\theta_j^T x)}$$\n",
                "\n",
                "**Key Properties:**\n",
                "1. The gradient is just as elegant as Logistic: $(h-y)x$.\n",
                "2. Output always sums to 1.0.\n",
                "\n",
                "Check this out - this is one of those beautiful moments where the math just works out perfectly!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“ 3. Gradient Derivation Step-by-Step\n",
                "\n",
                "### Cross-Entropy Loss (Multi-class)\n",
                "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K \\mathbf{1}\\{y^{(i)}=k\\} \\log P(y^{(i)}=k|x^{(i)})$$\n",
                "\n",
                "Now here's where indicator notation makes everything cleaner â€“ the gradient simplifies to:\n",
                "$$\\frac{\\partial J}{\\partial \\theta_k} = \\frac{1}{m} X^T (P_k - \\mathbf{1}\\{y=k\\})$$\n",
                "\n",
                "\n",
                "See how similar this is to Logistic Regression? That's not a coincidence â€“ it's the same underlying structure!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ§® 3.5 Manual Calculation: 1 Iteration Step-by-Step\n",
                "\n",
                "Alright, now for the fun part â€“ let's compute **BY HAND** 1 iteration of Softmax Regression for 3 classes. This is where the intuition really clicks!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ“ MANUAL CALCULATION: SOFTMAX REGRESSION\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import numpy as np\n",
                "\n",
                "print('ğŸ§® MANUAL SOFTMAX CALCULATION')\n",
                "print('='*70)\n",
                "\n",
                "# Tiny dataset: 2 samples, 2 features, 3 classes\n",
                "X_tiny = np.array([[1, 2],\n",
                "                   [2, 3]])\n",
                "y_tiny = np.array([0, 2])  # Class indices\n",
                "\n",
                "# Add bias\n",
                "X_tiny_b = np.c_[np.ones((2, 1)), X_tiny]  # [1, x1, x2]\n",
                "\n",
                "# Initialize theta: (K x (n+1)) = (3 x 3)\n",
                "# Each row = weights for 1 class\n",
                "theta = np.zeros((3, 3))  # 3 classes, 3 features (with bias)\n",
                "\n",
                "alpha = 0.1  # learning rate\n",
                "m = 2  # number of samples\n",
                "\n",
                "print(f'Data: X_tiny shape = {X_tiny.shape}')\n",
                "print(f'Labels: y = {y_tiny}')\n",
                "print(f'Theta shape: {theta.shape} (K x n+1)')\n",
                "print(f'Learning rate: {alpha}')\n",
                "print()\n",
                "\n",
                "# STEP 1: Compute logits z = X @ theta.T\n",
                "z = X_tiny_b @ theta.T  # (2 x 3) @ (3 x 3).T = (2 x 3)\n",
                "print('STEP 1: Compute logits z = X @ theta.T')\n",
                "print(f'  z shape: {z.shape} (m x K)')\n",
                "print(f'  z = \\n{z}')\n",
                "print('  (Each row = logits for 1 sample, K columns = K classes)')\n",
                "print()\n",
                "\n",
                "# STEP 2: Apply Softmax\n",
                "def softmax_manual(z):\n",
                "    # Subtract max for numerical stability\n",
                "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
                "    exp_z = np.exp(z_shifted)\n",
                "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
                "\n",
                "probs = softmax_manual(z)\n",
                "print('STEP 2: Apply Softmax')\n",
                "print(f'  Probabilities shape: {probs.shape}')\n",
                "print(f'  Probabilities = \\n{probs}')\n",
                "print(f'  Sum per row: {probs.sum(axis=1)} (should be [1.0, 1.0])')\n",
                "print()\n",
                "\n",
                "# STEP 3: One-hot encode labels\n",
                "y_onehot = np.zeros((m, 3))\n",
                "y_onehot[np.arange(m), y_tiny] = 1\n",
                "print('STEP 3: One-hot encode labels')\n",
                "print(f'  y_onehot = \\n{y_onehot}')\n",
                "print('  (Row 0: [1,0,0] = class 0, Row 1: [0,0,1] = class 2)')\n",
                "print()\n",
                "\n",
                "# STEP 4: Compute gradient\n",
                "gradient = (1/m) * X_tiny_b.T @ (probs - y_onehot)  # (3 x 2) @ (2 x 3) = (3 x 3)\n",
                "print('STEP 4: Compute gradient = (1/m) * X.T @ (probs - y_onehot)')\n",
                "print(f'  Gradient shape: {gradient.shape} (n+1 x K)')\n",
                "print(f'  Gradient = \\n{gradient}')\n",
                "print()\n",
                "\n",
                "# STEP 5: Update theta\n",
                "theta_new = theta - alpha * gradient.T  # Transpose to match (K x n+1)\n",
                "print('STEP 5: Update theta = theta - alpha * gradient.T')\n",
                "print(f'  Theta_new = \\n{theta_new}')\n",
                "print()\n",
                "\n",
                "# STEP 6: Compute loss\n",
                "epsilon = 1e-15\n",
                "probs_clip = np.clip(probs, epsilon, 1-epsilon)\n",
                "loss = -(1/m) * np.sum(y_onehot * np.log(probs_clip))\n",
                "print('STEP 6: Compute Cross-Entropy Loss')\n",
                "print(f'  Loss = {loss:.4f}')\n",
                "print()\n",
                "\n",
                "print('='*70)\n",
                "print('ğŸ’¡ After 1 iteration, theta is updated!')\n",
                "print('   This process repeats until convergence')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ” How to Read the Manual Calculation:\n",
                "\n",
                "**STEP 1: Logits (z)**\n",
                "- $z = X \\theta^T$ = Linear combination for each class\n",
                "- Shape: (m x K) - each sample has K logits\n",
                "\n",
                "**STEP 2: Softmax**\n",
                "- $P(y=k|x) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$\n",
                "- Normalize logits into probabilities\n",
                "- Sum per row = 1.0 âœ…\n",
                "\n",
                "**STEP 3: One-Hot Encoding**\n",
                "- y=0 â†’ [1, 0, 0]\n",
                "- y=1 â†’ [0, 1, 0]\n",
                "- y=2 â†’ [0, 0, 1]\n",
                "- Needed to compute gradient\n",
                "\n",
                "**STEP 4: Gradient**\n",
                "- $\\nabla J = \\frac{1}{m} X^T (probs - y_{onehot})$\n",
                "- Error = Predicted probabilities - True labels\n",
                "- Shape: (n+1 x K)\n",
                "\n",
                "**STEP 5: Update**\n",
                "- $\\theta := \\theta - \\alpha \\nabla J^T$\n",
                "- Move weights to minimize loss\n",
                "\n",
                "**STEP 6: Loss**\n",
                "- Categorical Cross-Entropy\n",
                "- Goal: Minimize loss â†’ better predictions\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ”¬ 3.6 Gradient Check\n",
                "\n",
                "Here's the thing - we always need to verify our implementation with numerical gradient. Let's make sure our math checks out!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ”¬ GRADIENT CHECK FOR SOFTMAX\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "def softmax_gc(z):\n",
                "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
                "    exp_z = np.exp(z_shifted)\n",
                "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
                "\n",
                "def compute_loss_gc(theta, X, y, K):\n",
                "    \"\"\"Categorical Cross-Entropy\"\"\"\n",
                "    m = len(y)\n",
                "    # theta shape: (K x n+1), need to reshape\n",
                "    theta_matrix = theta.reshape(K, -1)\n",
                "    \n",
                "    z = X @ theta_matrix.T\n",
                "    probs = softmax_gc(z)\n",
                "    \n",
                "    # One-hot encode\n",
                "    y_onehot = np.zeros((m, K))\n",
                "    y_onehot[np.arange(m), y] = 1\n",
                "    \n",
                "    epsilon = 1e-15\n",
                "    probs_clip = np.clip(probs, epsilon, 1-epsilon)\n",
                "    \n",
                "    return -(1/m) * np.sum(y_onehot * np.log(probs_clip))\n",
                "\n",
                "def numerical_gradient_softmax(theta, X, y, K, epsilon=1e-7):\n",
                "    \"\"\"Numerical gradient\"\"\"\n",
                "    num_grad = np.zeros_like(theta)\n",
                "    \n",
                "    for i in range(len(theta)):\n",
                "        theta_plus = theta.copy()\n",
                "        theta_minus = theta.copy()\n",
                "        theta_plus[i] += epsilon\n",
                "        theta_minus[i] -= epsilon\n",
                "        \n",
                "        loss_plus = compute_loss_gc(theta_plus, X, y, K)\n",
                "        loss_minus = compute_loss_gc(theta_minus, X, y, K)\n",
                "        \n",
                "        num_grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
                "    \n",
                "    return num_grad\n",
                "\n",
                "def analytical_gradient_softmax(theta, X, y, K):\n",
                "    \"\"\"Analytical gradient\"\"\"\n",
                "    m = len(y)\n",
                "    theta_matrix = theta.reshape(K, -1)\n",
                "    \n",
                "    z = X @ theta_matrix.T\n",
                "    probs = softmax_gc(z)\n",
                "    \n",
                "    # One-hot\n",
                "    y_onehot = np.zeros((m, K))\n",
                "    y_onehot[np.arange(m), y] = 1\n",
                "    \n",
                "    # Gradient\n",
                "    gradient = (1/m) * X.T @ (probs - y_onehot)  # (n+1 x K)\n",
                "    \n",
                "    return gradient.T.flatten()  # Flatten to match theta shape\n",
                "\n",
                "# Test\n",
                "np.random.seed(42)\n",
                "X_test = np.c_[np.ones((5, 1)), np.random.randn(5, 2)]\n",
                "y_test = np.random.randint(0, 3, 5)\n",
                "K_test = 3\n",
                "theta_test = np.random.randn(K_test * X_test.shape[1]) * 0.01\n",
                "\n",
                "num_grad = numerical_gradient_softmax(theta_test, X_test, y_test, K_test)\n",
                "ana_grad = analytical_gradient_softmax(theta_test, X_test, y_test, K_test)\n",
                "\n",
                "print('ğŸ”¬ GRADIENT CHECK (SOFTMAX)')\n",
                "print('='*50)\n",
                "print(f'Numerical Gradient (first 5):  {num_grad[:5]}')\n",
                "print(f'Analytical Gradient (first 5): {ana_grad[:5]}')\n",
                "print(f'Difference: {np.linalg.norm(num_grad - ana_grad):.2e}')\n",
                "print('='*50)\n",
                "\n",
                "if np.linalg.norm(num_grad - ana_grad) < 1e-5:\n",
                "    print('âœ… Gradient implementation is CORRECT!')\n",
                "else:\n",
                "    print('âŒ WARNING: Gradient mismatch!')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ” How to Read Softmax Output:\n",
                "\n",
                "**Softmax transforms logits into probabilities:**\n",
                "\n",
                "| Input (logits) | Output (probabilities) | Interpretation |\n",
                "|----------------|------------------------|-------------|\n",
                "| [2.0, 1.0, 0.1] | [0.66, 0.24, 0.10] | Class 0 most likely (66%) |\n",
                "| [5.0, 5.0, 5.0] | [0.33, 0.33, 0.33] | All classes equally likely (uncertain) |\n",
                "| [10, 0, 0] | [0.9999, 0.0, 0.0] | Very confident class 0 |\n",
                "\n",
                "**IF:**\n",
                "- **Max probability > 0.8:** Model very confident âœ…\n",
                "- **Max probability < 0.4:** Model uncertain âš ï¸\n",
                "- **All probabilities close:** Need more data/features\n",
                "\n",
                "**ğŸ’¡ Key:** Î£ probabilities = 1.0 (different from multiple binary classifiers!)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ”¬ 4. Softmax Formula Anatomy\n",
                "\n",
                "$$P(y=k|x) = \\frac{e^{\\theta_k^T x}}{\\sum_j e^{\\theta_j^T x}}$$\n",
                "\n",
                "Now here's where it gets interesting - **Why Exponential?** To ensure values are always positive (non-negative) and amplify the differences between scores (logits). Pretty elegant, right?\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ’» 5. Implementation from Scratch (Numba Accelerated)\n",
                "\n",
                "Check this out - we're building this from the ground up with performance optimization!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ’» 5.1 Full Softmax Class (From Scratch)\n",
                "\n",
                "Here's the thing - we're building a complete implementation with OOP style. This is production-ready code!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ—ï¸ SOFTMAX REGRESSION CLASS (FROM SCRATCH)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import numpy as np\n",
                "\n",
                "class SoftmaxRegressionScratch:\n",
                "    def __init__(self, learning_rate=0.01, max_iter=100, batch_size=None,\n",
                "                 regularization='none', lambda_reg=0.01):\n",
                "        self.lr = learning_rate\n",
                "        self.max_iter = n_epochs\n",
                "        self.batch_size = batch_size\n",
                "        self.regularization = regularization\n",
                "        self.lambda_reg = lambda_reg\n",
                "        self.theta = None\n",
                "        self.loss_history = []\n",
                "    \n",
                "    def _softmax(self, z):\n",
                "        \"\"\"Softmax activation with numerical stability\"\"\"\n",
                "        z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
                "        exp_z = np.exp(z_shifted)\n",
                "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
                "    \n",
                "    def _compute_loss(self, X, y, theta, K):\n",
                "        \"\"\"Categorical Cross-Entropy Loss\"\"\"\n",
                "        m = len(y)\n",
                "        theta_matrix = theta.reshape(K, -1)\n",
                "        \n",
                "        z = X @ theta_matrix.T\n",
                "        probs = self._softmax(z)\n",
                "        \n",
                "        # One-hot encode\n",
                "        y_onehot = np.zeros((m, K))\n",
                "        y_onehot[np.arange(m), y] = 1\n",
                "        \n",
                "        epsilon = 1e-15\n",
                "        probs_clip = np.clip(probs, epsilon, 1-epsilon)\n",
                "        \n",
                "        loss = -(1/m) * np.sum(y_onehot * np.log(probs_clip))\n",
                "        \n",
                "        # Regularization\n",
                "        if self.regularization == 'l2':\n",
                "            # Don't regularize bias\n",
                "            loss += (self.lambda_reg / (2*m)) * np.sum(theta_matrix[:, 1:]**2)\n",
                "        \n",
                "        return loss\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"Train the model\"\"\"\n",
                "        m, n = X.shape\n",
                "        self.K = len(np.unique(y))\n",
                "        \n",
                "        # Add bias\n",
                "        X_b = np.c_[np.ones((m, 1)), X]\n",
                "        \n",
                "        # Initialize theta: (K x (n+1))\n",
                "        self.theta = np.zeros((self.K, n + 1))\n",
                "        theta_flat = self.theta.flatten()\n",
                "        \n",
                "        # Gradient Descent\n",
                "        for epoch in range(self.max_iter):\n",
                "            # Shuffle\n",
                "            indices = np.random.permutation(m)\n",
                "            X_b_shuffled = X_b[indices]\n",
                "            y_shuffled = y[indices]\n",
                "            \n",
                "            bs = self.batch_size if self.batch_size else m\n",
                "            \n",
                "            for i in range(0, m, bs):\n",
                "                xi = X_b_shuffled[i:i+bs]\n",
                "                yi = y_shuffled[i:i+bs]\n",
                "                \n",
                "                # Forward\n",
                "                theta_matrix = self.theta\n",
                "                z = xi @ theta_matrix.T\n",
                "                probs = self._softmax(z)\n",
                "                \n",
                "                # One-hot\n",
                "                y_onehot = np.zeros((len(yi), self.K))\n",
                "                y_onehot[np.arange(len(yi)), yi] = 1\n",
                "                \n",
                "                # Gradient\n",
                "                gradient = (1/len(yi)) * xi.T @ (probs - y_onehot)\n",
                "                \n",
                "                # Regularization gradient\n",
                "                if self.regularization == 'l2':\n",
                "                    reg_grad = np.zeros_like(gradient)\n",
                "                    reg_grad[1:, :] = (self.lambda_reg / len(yi)) * theta_matrix[:, 1:].T\n",
                "                    gradient += reg_grad\n",
                "                \n",
                "                # Update\n",
                "                self.theta -= self.lr * gradient.T\n",
                "            \n",
                "            # Log loss\n",
                "            loss = self._compute_loss(X_b, y, self.theta.flatten(), self.K)\n",
                "            self.loss_history.append(loss)\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def predict_proba(self, X):\n",
                "        \"\"\"Return class probabilities\"\"\"\n",
                "        X_b = np.c_[np.ones((len(X), 1)), X]\n",
                "        z = X_b @ self.theta.T\n",
                "        return self._softmax(z)\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"Return class predictions\"\"\"\n",
                "        probs = self.predict_proba(X)\n",
                "        return np.argmax(probs, axis=1)\n",
                "\n",
                "print('âœ… SoftmaxRegressionScratch class created!')\n",
                "print('   Features: BGD/SGD/Mini-Batch, L2 Regularization')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn import datasets\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import seaborn as sns\n",
                "try:\n",
                "    from numba import njit\n",
                "    HAS_NUMBA = True\n",
                "except ImportError:\n",
                "    HAS_NUMBA = False\n",
                "    def njit(func): return func\n",
                "\n",
                "@njit\n",
                "def _softmax_fast(logits):\n",
                "    c = np.max(logits, axis=1).reshape(-1, 1)\n",
                "    exps = np.exp(logits - c)\n",
                "    return exps / np.sum(exps, axis=1).reshape(-1, 1)\n",
                "\n",
                "class SoftmaxRegressionScratch:\n",
                "    def __init__(self, learning_rate=0.1, n_iterations=1000, batch_size=None, \n",
                "                 lambda_reg=0.01, tol=1e-5, patience=10, class_weight=None):\n",
                "        self.lr = learning_rate\n",
                "        self.n_iterations = n_iterations\n",
                "        self.batch_size = batch_size\n",
                "        self.lambda_reg = lambda_reg\n",
                "        self.tol = tol\n",
                "        self.patience = patience\n",
                "        self.class_weight = class_weight\n",
                "        self.theta = None\n",
                "        self.cost_history = []\n",
                "\n",
                "    def to_one_hot(self, y, k):\n",
                "        m = len(y)\n",
                "        one_hot = np.zeros((m, k))\n",
                "        one_hot[np.arange(m), y] = 1\n",
                "        return one_hot\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        m, n = X.shape\n",
                "        k = len(np.unique(y))\n",
                "        self.theta = np.zeros((n, k))\n",
                "        y_oh = self.to_one_hot(y, k)\n",
                "        weights = np.ones(m)\n",
                "        if self.class_weight == 'balanced':\n",
                "            counts = np.bincount(y)\n",
                "            w = m / (k * counts)\n",
                "            weights = w[y]\n",
                "        best_cost = np.inf\n",
                "        wait = 0\n",
                "        bs = self.batch_size if self.batch_size else m\n",
                "        for i in range(self.n_iterations):\n",
                "            idx = np.random.permutation(m)\n",
                "            X_s, y_s, w_s = X[idx], y_oh[idx], weights[idx]\n",
                "            for j in range(0, m, bs):\n",
                "                xb, yb, wb = X_s[j:j+bs], y_s[j:j+bs], w_s[j:j+bs]\n",
                "                probs = _softmax_fast(np.dot(xb, self.theta))\n",
                "                error = (probs - yb) * wb.reshape(-1, 1)\n",
                "                grad = (1/len(yb)) * np.dot(xb.T, error) + self.lambda_reg * self.theta\n",
                "                self.theta -= self.lr * grad\n",
                "            full_probs = _softmax_fast(np.dot(X, self.theta))\n",
                "            cost = -np.mean(np.sum(y_oh * np.log(full_probs + 1e-15), axis=1)) + (self.lambda_reg / 2) * np.sum(self.theta**2)\n",
                "            self.cost_history.append(cost)\n",
                "            if cost < best_cost - self.tol: best_cost, wait = cost, 0\n",
                "            else:\n",
                "                wait += 1\n",
                "                if wait >= self.patience: break\n",
                "\n",
                "    def predict_proba(self, X):\n",
                "        return _softmax_fast(np.dot(X, self.theta))\n",
                "\n",
                "    def predict(self, X):\n",
                "        return np.argmax(self.predict_proba(X), axis=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“Š 6. Evaluation & Visualization\n",
                "\n",
                "Now let's see how our model performs in the real world!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
                "iris = datasets.load_iris()\n",
                "X, y = iris.data, iris.target\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
                "scaler = StandardScaler()\n",
                "X_train_s, X_test_s = scaler.fit_transform(X_train), scaler.transform(X_test)\n",
                "X_train_b = np.c_[np.ones(len(X_train_s)), X_train_s]\n",
                "X_test_b = np.c_[np.ones(len(X_test_s)), X_test_s]\n",
                "model = SoftmaxRegressionScratch(lambda_reg=0.01)\n",
                "model.fit(X_train_b, y_train)\n",
                "y_pred = model.predict(X_test_b)\n",
                "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
                "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
                "plt.title('Normalized Confusion Matrix')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸŒ¡ï¸ TEMPERATURE TUNING DEMO\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "def softmax_temp(logits, T=1.0):\n",
                "    \"\"\"Softmax with temperature\"\"\"\n",
                "    z = logits / T\n",
                "    exp_z = np.exp(z - np.max(z))\n",
                "    return exp_z / exp_z.sum()\n",
                "\n",
                "# Example logits\n",
                "logits = np.array([2.0, 1.0, 0.5])\n",
                "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "for i, T in enumerate(temperatures):\n",
                "    probs = softmax_temp(logits, T)\n",
                "    \n",
                "    plt.subplot(1, 4, i+1)\n",
                "    plt.bar(['Class 0', 'Class 1', 'Class 2'], probs, color=['b', 'g', 'r'], alpha=0.7)\n",
                "    plt.ylim(0, 1)\n",
                "    plt.title(f'Temperature = {T}')\n",
                "    plt.ylabel('Probability')\n",
                "    \n",
                "    # Annotate values\n",
                "    for j, p in enumerate(probs):\n",
                "        plt.text(j, p + 0.05, f'{p:.2f}', ha='center')\n",
                "\n",
                "plt.suptitle('ğŸŒ¡ï¸ Effect of Temperature on Softmax Probabilities', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print('ğŸ“Š Temperature Effect Summary:')\n",
                "print(f'Original logits: {logits}')\n",
                "print()\n",
                "for T in temperatures:\n",
                "    probs = softmax_temp(logits, T)\n",
                "    print(f'T={T:4.1f}: {probs} (max={probs.max():.2f})')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ” 7. Industry Implementation & IF-THEN Patterns\n",
                "\n",
                "Check this out - this is how the pros use it in production!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ­ Industry Examples\n",
                "1. **Gmail Categorization**: Primary, Social, Promotions.\n",
                "2. **Medical Staging**: Stage I, II, III.\n",
                "\n",
                "### ğŸ” IF-THEN Diagnostics\n",
                "| Condition (IF) | Interpretation (THEN) | Action |\n",
                "|----------------|---------------------|------|\n",
                "| Max Prob < 0.5 | Model Uncertain | Review data quality |\n",
                "| Macro-F1 << Weighted-F1 | Bias toward majority | Activate `class_weight='balanced'` |\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ”¬ 8. Advanced Research Techniques\n",
                "\n",
                "Now here's where it gets interesting - these are cutting-edge techniques used in modern research!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Label Smoothing\n",
                "def smooth_labels(y_oh, e=0.1):\n",
                "    return y_oh * (1-e) + (e/y_oh.shape[1])\n",
                "\n",
                "# 2. Temperature Scaling\n",
                "def softmax_temp(logits, T=1.0):\n",
                "    # Note: _softmax_fast is defined in PART 5\n",
                "    return _softmax_fast(logits/T)\n",
                "\n",
                "# 3. Monte Carlo Dropout (Uncertainty Estimation)\n",
                "def mc_dropout_predict(model, X, n_samples=30):\n",
                "    results = []\n",
                "    for _ in range(n_samples):\n",
                "        # Simulate dropout on weights (Simple weights noise)\n",
                "        noise = np.random.normal(0, 0.05, model.theta.shape)\n",
                "        res = _softmax_fast(np.dot(X, model.theta + noise))\n",
                "        results.append(res)\n",
                "    mean_prob = np.mean(results, axis=0)\n",
                "    std_prob = np.std(results, axis=0)\n",
                "    return mean_prob, std_prob\n",
                "\n",
                "p_mean, p_std = mc_dropout_predict(model, X_test_b[:1])\n",
                "print(\"Mean Prediction Probs:\", p_mean)\n",
                "print(\"Uncertainty (Stdev) per class:\", p_std)\n",
                "\n",
                "# 4. Knowledge Distillation (Conceptual)\n",
                "print(\"Distillation Tip: Train student by minimizing: alpha * CrossEntropy(y_true, student) + (1-alpha) * CrossEntropy(teacher_probs_soft, student_soft)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ” How to Read Softmax Output:\n",
                "\n",
                "**Softmax transforms logits into probabilities:**\n",
                "\n",
                "| Input (logits) | Output (probabilities) | Interpretation |\n",
                "|----------------|------------------------|-------------|\n",
                "| [2.0, 1.0, 0.1] | [0.66, 0.24, 0.10] | Class 0 most likely (66%) |\n",
                "| [5.0, 5.0, 5.0] | [0.33, 0.33, 0.33] | All classes equally likely (uncertain) |\n",
                "| [10, 0, 0] | [0.9999, 0.0, 0.0] | Very confident class 0 |\n",
                "\n",
                "**IF:**\n",
                "- **Max probability > 0.8:** Model very confident âœ…\n",
                "- **Max probability < 0.4:** Model uncertain âš ï¸\n",
                "- **All probabilities close:** Need more data/features\n",
                "\n",
                "**ğŸ’¡ Key:** Î£ probabilities = 1.0 (different from multiple binary classifiers!)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“ 9. Capstone & Exercises\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 1: Temperature Tuning\n",
                "Observe probability changes as T approaches 0. What happens?\n",
                "\n",
                "### Deployment Case Study: FastAPI\n",
                "Use `model.predict_proba` to return Top-K categories in the API.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ“ CAPSTONE PROJECT\n",
                "**Objective:** Build a News Article Classifier using Softmax from Scratch + TF-IDF.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“Š 5.2 Optimizer Comparison (Multi-Class)\n",
                "\n",
                "Check this out - let's compare BGD, SGD, and Mini-Batch for Softmax Regression. This is where performance matters!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ OPTIMIZER COMPARISON FOR SOFTMAX\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Generate 3-class dataset\n",
                "X, y = make_classification(n_samples=600, n_features=10, n_informative=8,\n",
                "                          n_redundant=2, n_classes=3, n_clusters_per_class=1,\n",
                "                          random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "print(f'Dataset: {X_train.shape[0]} train, {X_test.shape[0]} test, {len(np.unique(y))} classes')\n",
                "print()\n",
                "\n",
                "# Train different optimizers\n",
                "models = [\n",
                "    ('Batch GD', SoftmaxRegressionScratch(learning_rate=0.1, max_iter=50, batch_size=None)),\n",
                "    ('SGD', SoftmaxRegressionScratch(learning_rate=0.1, max_iter=50, batch_size=1)),\n",
                "    ('Mini-Batch (16)', SoftmaxRegressionScratch(learning_rate=0.1, max_iter=50, batch_size=16)),\n",
                "    ('Mini-Batch (32)', SoftmaxRegressionScratch(learning_rate=0.1, max_iter=50, batch_size=32)),\n",
                "]\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "for name, model in models:\n",
                "    model.fit(X_train, y_train)\n",
                "    plt.plot(model.loss_history, label=name, linewidth=2)\n",
                "    \n",
                "    # Evaluate\n",
                "    y_pred = model.predict(X_test)\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    print(f'{name:20} - Test Accuracy: {acc:.4f}')\n",
                "\n",
                "plt.xlabel('Epoch', fontsize=12)\n",
                "plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
                "plt.title('ğŸ Optimizer Comparison: Softmax Regression (3 Classes)', fontsize=14)\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ” How to Read Optimizer Comparison:\n",
                "\n",
                "**IF Batch GD:**\n",
                "- Loss curve is **smooth** but convergence is **slow**\n",
                "- Needs many epochs for multi-class\n",
                "\n",
                "**IF SGD:**\n",
                "- Loss curve is **noisy** (zigzag)\n",
                "- Fast updates but unstable\n",
                "\n",
                "**IF Mini-Batch:**\n",
                "- **Best balance** between speed & stability\n",
                "- Batch size 16-32 optimal for most cases\n",
                "\n",
                "**ğŸ’¡ Production Tip:** Mini-Batch (32) + L2 regularization = best default!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# ğŸ”¥ CONFUSION MATRIX HEATMAP\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "from sklearn.metrics import confusion_matrix\n",
                "import seaborn as sns\n",
                "\n",
                "# Use best model from optimizer comparison\n",
                "best_model = SoftmaxRegressionScratch(learning_rate=0.1, max_iter=100, batch_size=32)\n",
                "best_model.fit(X_train, y_train)\n",
                "y_pred = best_model.predict(X_test)\n",
                "\n",
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
                "           xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
                "           yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
                "plt.xlabel('Predicted Class', fontsize=12)\n",
                "plt.ylabel('Actual Class', fontsize=12)\n",
                "plt.title('ğŸ”¥ Confusion Matrix Heatmap (Softmax)', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print accuracy per class\n",
                "print('\\nğŸ“Š Per-Class Accuracy:')\n",
                "for i in range(3):\n",
                "    class_acc = cm[i, i] / cm[i].sum()\n",
                "    print(f'  Class {i}: {class_acc:.4f} ({cm[i, i]}/{cm[i].sum()} samples)')\n",
                "\n",
                "print(f'\\n Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ” How to Read Confusion Heatmap:\n",
                "\n",
                "**Color Intensity:**\n",
                "- **Dark Blue (Diagonal):** High correct predictions âœ…\n",
                "- **Light Blue (Off-diagonal):** Misclassifications âŒ\n",
                "\n",
                "**IF Diagonal is Dark, Off-diagonal is Light:**\n",
                "- **THEN:** Good classifier!\n",
                "- Most predictions are correct\n",
                "\n",
                "**IF There's a Dark Off-diagonal:**\n",
                "- **THEN:** Specific confusion pattern\n",
                "- Example: Class 1 often confused with Class 2\n",
                "- **ACTION:** Add features that distinguish the two classes\n",
                "\n",
                "**IF One Row Has Many Errors:**\n",
                "- **THEN:** That class is hard to predict\n",
                "- **ACTION:** Collect more data for that class\n",
                "\n",
                "**ğŸ’¡ Production:** Monitor confusion matrix in production to detect data drift!\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

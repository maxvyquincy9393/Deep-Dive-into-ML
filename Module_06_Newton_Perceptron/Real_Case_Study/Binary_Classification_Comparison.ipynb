{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Studi Kasus: Perbandingan Kecepatan Deteksi Kanker\n\n",
    "---\n\n",
    "## Tantangan Kecepatan\n\n",
    "During melatih model ML, **kecepatan konvergensi** penting:\n",
    "- Training lebih cepat = eksperimen lebih cepat\n",
    "- Deployment lebih cepat = nilai lebih cepat\n\n",
    "Mari bandingkan:\n",
    "- **Perceptron**: Sederhana, orde-pertama (pakai gradien)\n",
    "- **Newton's Method**: Orde-kedua (pakai matriks Hessian)\n\n",
    "---\n\n",
    "## Dataset: Breast Cancer Wisconsin\n\n",
    "569 pasien with 30 pengukuran klinis. Tujuan: Jinak atau Ganas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "print(\"Memuat Breast Cancer Dataset...\")\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "print(f\"âœ… {len(X)} pasien, {X.shape[1]} feature\")\n",
    "print(f\"   class: {cancer.target_names}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandingkan kecepatan training and performa\n",
    "hasil = {}\n",
    "\n",
    "# Perceptron\n",
    "mulai = time.time()\n",
    "perceptron = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron.fit(X_train, y_train)\n",
    "hasil['Perceptron'] = {\n",
    "    'waktu': (time.time() - mulai) * 1000,\n",
    "    'akurasi': accuracy_score(y_test, perceptron.predict(X_test)),\n",
    "    'iterasi': perceptron.n_iter_\n",
    "}\n",
    "\n",
    "# Newton-CG\n",
    "mulai = time.time()\n",
    "newton = LogisticRegression(solver='newton-cg', max_iter=1000)\n",
    "newton.fit(X_train, y_train)\n",
    "hasil['Newton-CG'] = {\n",
    "    'waktu': (time.time() - mulai) * 1000,\n",
    "    'akurasi': accuracy_score(y_test, newton.predict(X_test)),\n",
    "    'iterasi': newton.n_iter_[0]\n",
    "}\n",
    "\n",
    "# LBFGS (Quasi-Newton)\n",
    "mulai = time.time()\n",
    "lbfgs = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lbfgs.fit(X_train, y_train)\n",
    "hasil['LBFGS'] = {\n",
    "    'waktu': (time.time() - mulai) * 1000,\n",
    "    'akurasi': accuracy_score(y_test, lbfgs.predict(X_test)),\n",
    "    'iterasi': lbfgs.n_iter_[0]\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š Hasil:\")\n",
    "print(f\"{'Model':<15} {'Waktu (ms)':<12} {'Akurasi':<12} {'Iterasi':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for nama, data in hasil.items():\n",
    "    print(f\"{nama:<15} {data['waktu']:<12.2f} {data['akurasi']*100:<12.1f}% {data['iterasi']:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "nama_model = list(hasil.keys())\n",
    "warna = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "axes[0].barh(nama_model, [r['waktu'] for r in hasil.values()], color=warna)\n",
    "axes[0].set_xlabel('Waktu Training (ms)')\n",
    "axes[0].set_title('Kecepatan Training')\n",
    "\n",
    "axes[1].barh(nama_model, [r['iterasi'] for r in hasil.values()], color=warna)\n",
    "axes[1].set_xlabel('Iterasi sampai Konvergen')\n",
    "axes[1].set_title('Kecepatan Konvergensi')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Insight Kunci:\")\n",
    "print(\"   Metode berbasis Newton konvergen dalam iterasi LEBIH SEDIKIT\")\n",
    "print(\"   Info orde-kedua (Hessian) givekan arah yang lebih baik!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Conclusion\n\n",
    "1. **Metode orde-pertama** (Perceptron, SGD): Sederhana, iterasi lebih banyak\n",
    "2. **Metode orde-kedua** (Newton): Iterasi lebih sedikit, tapi tiap iterasi lebih mahal\n",
    "3. **Quasi-Newton** (LBFGS): Terbaik dari dua dunia - konvergensi cepat, hemat memori\n",
    "4. for dataset besar, LBFGS sering jadi pilihan terbaik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Enhancements Added (Batch Process)\n",
    "\n",
    "- **Aggressive text cleaning** (phone numbers, URLs, slang, lemmatization)\n",
    "- **Scikitâ€‘Learn `Pipeline`** (`CountVectorizer` â†’ `MultinomialNB`)\n",
    "- **Imbalance handling** via `class_weight='balanced'` (or SMOTE if desired)\n",
    "- **Model persistence** with `joblib.dump`\n",
    "\n",
    "*These cells were inserted automatically to bring the notebook to productionâ€‘ready quality.*\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1ï¸âƒ£ Aggressive preprocessing (regex + lemmatization)\n",
    "# --------------------------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    import re\n",
    "    # phone numbers â†’ PHONE_NUMBER\n",
    "    text = re.sub(r\"\\b\\d{10,13}\\b\", \" PHONE_NUMBER \", text)\n",
    "    # URLs â†’ URL\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" URL \", text)\n",
    "    # simple slang map (extend as needed)\n",
    "    slang = {\"u\":\"you\",\"ur\":\"your\",\"btw\":\"by the way\",\"lol\":\"laugh\",\"thx\":\"thanks\"}\n",
    "    for k,v in slang.items():\n",
    "        text = re.sub(rf\"\\b{k}\\b\", v, text, flags=re.IGNORECASE)\n",
    "    return \" \".join(text.lower().split())\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2ï¸âƒ£ Scikitâ€‘Learn Pipeline (CountVectorizer + MultinomialNB)\n",
    "# --------------------------------------------------------------\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# class_weight='balanced' handles mild imbalance; replace with SMOTE if you have imbalancedâ€‘learn installed.\n",
    "model = make_pipeline(\n",
    "    CountVectorizer(preprocessor=preprocess_text, stop_words='english', max_features=5000),\n",
    "    MultinomialNB(alpha=1.0, class_weight='balanced')\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3ï¸âƒ£ Train / Evaluate (example â€“ adapt to your data)\n",
    "# --------------------------------------------------------------\n",
    "# X_train, X_test, y_train, y_test must be defined in the notebook\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham','Spam']))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4ï¸âƒ£ Persist model for production use\n",
    "# --------------------------------------------------------------\n",
    "import joblib\n",
    "joblib.dump(model, 'naive_bayes_pipeline.pkl')\n",
    "print(\"Model saved to naive_bayes_pipeline.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
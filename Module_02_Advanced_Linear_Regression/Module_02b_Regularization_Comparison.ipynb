{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0f705",
   "metadata": {},
   "source": [
    "## The Three Regularization Philosophies\n",
    "\n",
    "All three methods add a penalty to the ordinary least squares cost function:\n",
    "\n",
    "**Ordinary Least Squares (no regularization):**\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Ridge Regression (L2 penalty):**\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "**Lasso Regression (L1 penalty):**\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$$\n",
    "\n",
    "**Elastic Net (L1 + L2):**\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57db11",
   "metadata": {},
   "source": [
    "## Experiment 1: Coefficient Shrinkage Visualization\n",
    "\n",
    "Let's create a dataset with many features and see how each regularization method shrinks the coefficients as we increase the penalty strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with 10 features, only 5 are truly informative\n",
    "X, y, true_coef = make_regression(n_samples=100, n_features=10, n_informative=5,\n",
    "                                   noise=10, coef=True, random_state=42)\n",
    "\n",
    "# Standardize features (important for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Dataset created:\")\n",
    "print(f\"  Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
    "print(f\"  True informative features: 5 out of 10\")\n",
    "print(f\"\\nTrue coefficients: {np.round(true_coef, 2)}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(true_coef != 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track coefficients across different regularization strengths\n",
    "alphas = np.logspace(-3, 3, 100)\n",
    "\n",
    "ridge_coefs = []\n",
    "lasso_coefs = []\n",
    "elastic_coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha).fit(X_scaled, y)\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000).fit(X_scaled, y)\n",
    "    elastic = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000).fit(X_scaled, y)\n",
    "    \n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "    elastic_coefs.append(elastic.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "elastic_coefs = np.array(elastic_coefs)\n",
    "\n",
    "print(\"Coefficient paths computed for 100 alpha values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b76a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient paths\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    axes[0].plot(np.log10(alphas), ridge_coefs[:, i], label=f'Feature {i+1}')\n",
    "    axes[1].plot(np.log10(alphas), lasso_coefs[:, i], label=f'Feature {i+1}')\n",
    "    axes[2].plot(np.log10(alphas), elastic_coefs[:, i], label=f'Feature {i+1}')\n",
    "\n",
    "axes[0].set_title('Ridge (L2): Coefficients Shrink Smoothly', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('log(alpha)')\n",
    "axes[0].set_ylabel('Coefficient Value')\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Lasso (L1): Coefficients Hit Zero', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('log(alpha)')\n",
    "axes[1].set_ylabel('Coefficient Value')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_title('Elastic Net: Hybrid Behavior', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('log(alpha)')\n",
    "axes[2].set_ylabel('Coefficient Value')\n",
    "axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY OBSERVATIONS:\")\n",
    "print(\"  Ridge: Coefficients shrink gradually toward zero but NEVER reach exactly zero\")\n",
    "print(\"  Lasso: Coefficients are FORCED to zero - automatic feature selection!\")\n",
    "print(\"  Elastic Net: Some coefficients hit zero, but shrinkage is smoother than Lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee30bc",
   "metadata": {},
   "source": [
    "## Experiment 2: Feature Selection Comparison\n",
    "\n",
    "The key difference between Ridge and Lasso is **sparsity**. Lasso can set coefficients exactly to zero, effectively selecting a subset of features. Let's quantify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with moderate regularization\n",
    "alpha = 1.0\n",
    "\n",
    "ols = LinearRegression().fit(X_scaled, y)\n",
    "ridge = Ridge(alpha=alpha).fit(X_scaled, y)\n",
    "lasso = Lasso(alpha=alpha, max_iter=10000).fit(X_scaled, y)\n",
    "elastic = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000).fit(X_scaled, y)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COEFFICIENT COMPARISON (alpha = 1.0)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Feature':<10} {'True':<10} {'OLS':<10} {'Ridge':<10} {'Lasso':<10} {'Elastic':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Feature {i+1:<3} {true_coef[i]:<10.2f} {ols.coef_[i]:<10.2f} {ridge.coef_[i]:<10.2f} {lasso.coef_[i]:<10.2f} {elastic.coef_[i]:<10.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Non-zero':<10} {np.sum(true_coef != 0):<10} {np.sum(np.abs(ols.coef_) > 0.01):<10} {np.sum(np.abs(ridge.coef_) > 0.01):<10} {np.sum(np.abs(lasso.coef_) > 0.01):<10} {np.sum(np.abs(elastic.coef_) > 0.01):<10}\")\n",
    "\n",
    "print(\"\\nINTERPRETATION:\")\n",
    "print(f\"  - True model has {np.sum(true_coef != 0)} non-zero features\")\n",
    "print(f\"  - OLS and Ridge keep all {X.shape[1]} features (no selection)\")\n",
    "print(f\"  - Lasso selected {np.sum(np.abs(lasso.coef_) > 0.01)} features (sparse!)\")\n",
    "print(f\"  - Elastic Net selected {np.sum(np.abs(elastic.coef_) > 0.01)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05467a",
   "metadata": {},
   "source": [
    "## Experiment 3: Cross-Validation for Hyperparameter Selection\n",
    "\n",
    "How do we choose the right alpha? Cross-validation. We'll find the optimal regularization strength for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal alpha via cross-validation\n",
    "alphas_cv = np.logspace(-4, 2, 50)\n",
    "\n",
    "ridge_scores = []\n",
    "lasso_scores = []\n",
    "elastic_scores = []\n",
    "\n",
    "for alpha in alphas_cv:\n",
    "    ridge_cv = cross_val_score(Ridge(alpha=alpha), X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    lasso_cv = cross_val_score(Lasso(alpha=alpha, max_iter=10000), X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    elastic_cv = cross_val_score(ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000), X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    ridge_scores.append(-ridge_cv.mean())\n",
    "    lasso_scores.append(-lasso_cv.mean())\n",
    "    elastic_scores.append(-elastic_cv.mean())\n",
    "\n",
    "# Find best alphas\n",
    "best_ridge_alpha = alphas_cv[np.argmin(ridge_scores)]\n",
    "best_lasso_alpha = alphas_cv[np.argmin(lasso_scores)]\n",
    "best_elastic_alpha = alphas_cv[np.argmin(elastic_scores)]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMAL REGULARIZATION STRENGTH (5-Fold CV)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nRidge:      alpha = {best_ridge_alpha:.4f}, MSE = {min(ridge_scores):.2f}\")\n",
    "print(f\"Lasso:      alpha = {best_lasso_alpha:.4f}, MSE = {min(lasso_scores):.2f}\")\n",
    "print(f\"Elastic Net: alpha = {best_elastic_alpha:.4f}, MSE = {min(elastic_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba884aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.log10(alphas_cv), ridge_scores, 'b-', label='Ridge', linewidth=2)\n",
    "plt.plot(np.log10(alphas_cv), lasso_scores, 'r-', label='Lasso', linewidth=2)\n",
    "plt.plot(np.log10(alphas_cv), elastic_scores, 'g-', label='Elastic Net', linewidth=2)\n",
    "\n",
    "plt.axvline(x=np.log10(best_ridge_alpha), color='b', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=np.log10(best_lasso_alpha), color='r', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=np.log10(best_elastic_alpha), color='g', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xlabel('log(alpha)', fontsize=12)\n",
    "plt.ylabel('Mean Squared Error (CV)', fontsize=12)\n",
    "plt.title('Cross-Validation: Finding Optimal Regularization Strength', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"  - Too little regularization (left): Overfitting, high variance\")\n",
    "print(\"  - Too much regularization (right): Underfitting, high bias\")\n",
    "print(\"  - Sweet spot: Minimum of the CV curve (marked with dashed lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b9026",
   "metadata": {},
   "source": [
    "## When to Use Which Regularization?\n",
    "\n",
    "| Scenario | Recommended | Why |\n",
    "|----------|-------------|-----|\n",
    "| Many features, all potentially relevant | **Ridge** | Keeps all features, just shrinks them |\n",
    "| Many features, only some are relevant | **Lasso** | Automatic feature selection (sparsity) |\n",
    "| Correlated features + feature selection | **Elastic Net** | Groups correlated features together |\n",
    "| Interpretability is critical | **Lasso** | Sparse models are easier to explain |\n",
    "| Prediction accuracy is the only goal | **Ridge** or **Elastic Net** | More stable, less variance |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Ridge (L2):**\n",
    "- Shrinks coefficients smoothly toward zero\n",
    "- Never sets coefficients exactly to zero\n",
    "- Best when all features contribute something\n",
    "\n",
    "**Lasso (L1):**\n",
    "- Forces some coefficients to exactly zero\n",
    "- Automatic feature selection\n",
    "- Best when you suspect many features are irrelevant\n",
    "\n",
    "**Elastic Net:**\n",
    "- Combines L1 and L2 penalties\n",
    "- Handles correlated features better than pure Lasso\n",
    "- Best of both worlds, but has two hyperparameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

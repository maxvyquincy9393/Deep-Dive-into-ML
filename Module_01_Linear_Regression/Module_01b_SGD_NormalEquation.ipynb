{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 01b: SGD & Normal Equation\n\n**CS229 Aligned Curriculum** | *Gold Standard Edition*\n\n**CS229 Aligned Curriculum** | *Gold Standard Edition*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83c\udfaf TARGET PEMBELAJARAN\n",
                "\n",
                "| No | Skill | Cara Verifikasi |\n",
                "|---|---|---|\n",
                "| 1 | Bandingkan **Batch vs SGD** | Tahu pros/cons |\n",
                "| 2 | Derive **Normal Equation** | Step-by-step |\n",
                "| 3 | Paham **probabilistic interpretation** | Kenapa MSE? |\n",
                "| 4 | Paham **MLE = Least Squares** | Derive koneksi |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83c\udfad Analogi untuk Pemula: Mencari Titik Terendah di Lembah\n",
                "\n",
                "### \ud83c\udf1f Level 1: Berjalan Menuruni Bukit\n",
                "\n",
                "Bayangkan Anda tersesat di pegunungan pada malam hari. Tujuan Anda: **mencapai lembah terendah**.\n",
                "\n",
                "**1. Batch Gradient Descent (Hati-hati Banget):**\n",
                "Anda berhenti setiap langkah, cek seluruh peta (semua data), baru tentukan arah langkah berikutnya.\n",
                "- **Pro:** Akurat, pasti sampai.\n",
                "- **Con:** Lambat banget! Capek cek peta terus.\n",
                "\n",
                "**2. Stochastic GD (Nekat):**\n",
                "Anda langsung jalan berdasarkan satu jejak kaki di depan Anda (satu data point).\n",
                "- **Pro:** Cepat banget!\n",
                "- **Con:** Jalannya zig-zag, kadang malah naik sebentar.\n",
                "\n",
                "**3. Mini-Batch GD (Kompromi):**\n",
                "Anda cek beberapa jejak kaki terdekat (batch kecil), baru tentukan arah.\n",
                "- **Pro:** Cukup cepat, cukup akurat.\n",
                "- **Con:** Harus pilih ukuran batch yang pas.\n",
                "\n",
                "**4. Momentum (Pakai Skateboard):**\n",
                "Anda pakai skateboard, jadi kecepatan sebelumnya membawa Anda terus. Tidak berhenti tiba-tiba.\n",
                "- **Pro:** Lebih cepat melewati dataran datar, tidak terjebak lubang kecil.\n",
                "- **Con:** Bisa kebablasan kalau turunan curam.\n",
                "\n",
                "---\n",
                "\n",
                "### \ud83d\udcd6 Istilah Kunci untuk Pemula\n",
                "\n",
                "| Istilah | Arti Sederhana |\n",
                "| :--- | :--- |\n",
                "| **Learning Rate (\u03b1)** | Seberapa besar langkah kaki Anda. Terlalu besar = melompat-lompat. Terlalu kecil = lama sampai. |\n",
                "| **Epoch** | Satu kali jalan melewati SELURUH data training. |\n",
                "| **Batch Size** | Berapa data yang Anda lihat sebelum mengambil satu langkah. |\n",
                "| **Convergence** | Sudah sampai di titik terendah (atau cukup dekat). |\n",
                "| **Cost/Loss** | Seberapa jauh Anda dari tujuan. Makin kecil, makin bagus. |\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\ude80 PART 1: STOCHASTIC GRADIENT DESCENT\n\n### 1.1 Update Rule\n\n$$\\theta_j := \\theta_j - \\alpha (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n\n#### \ud83d\udde3\ufe0f CARA BACA:\n\n> \"Theta j di-update dengan mengurangi alpha kali error untuk data i kali xj i\"\n\n### 1.2 Perbandingan\n\n| Aspek | Batch GD | SGD |\n|---|---|---|\n| Data per update | Semua m | 1 |\n| Speed per update | LAMBAT | CEPAT |\n| Convergence | Smooth | Noisy (zigzag) |\n| Memori | O(m) | O(1) |\n\n### 1.3 Visualisasi Convergence\n\n```\nBATCH GD:                    SGD:\n\nJ(\u03b8)                         J(\u03b8)\n \u2502\\                           \u2502\n \u2502 \\                          \u2502 \\    /\\\n \u2502  \\                         \u2502  \\  /  \\/\\\n \u2502   \\                        \u2502   \\/      \\/\\\n \u2502    \\___                    \u2502           \\___\n \u2502        \\___                \u2502               \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 iter         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 iter\n\nSmooth descent                Noisy tapi lebih cepat \n                              sampai ke area minimum\n```\n\n### 1.4 Mini-Batch: Kompromi\n\n```\nfor each mini-batch of size B:\n    gradient = (1/B) \u03a3_batch (error \u00d7 x)\n    \u03b8 = \u03b8 - \u03b1 \u00d7 gradient\n```\n\nBiasanya B = 32, 64, 128, 256"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n\n## \ud83e\uddee PART 2: NORMAL EQUATION (Closed-Form Solution)\n\n### 2.1 Ide\n\nDaripada iterate dengan gradient descent, langsung SOLVE!\n\nSet gradient = 0, solve for \u03b8.\n\n### 2.2 Matrix Notation\n\nDesign matrix X (m \u00d7 n+1):\n$$X = \\begin{bmatrix} \u2014 (x^{(1)})^T \u2014 \\\\ \u2014 (x^{(2)})^T \u2014 \\\\ \\vdots \\\\ \u2014 (x^{(m)})^T \u2014 \\end{bmatrix}$$\n\nTarget vector y (m \u00d7 1):\n$$y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}$$\n\n### 2.3 Cost Function dalam Matrix Form\n\n$$J(\\theta) = \\frac{1}{2m}(X\\theta - y)^T(X\\theta - y)$$\n\n### 2.4 Derivasi Normal Equation (Step-by-Step)\n\n**STEP 1:** Tulis J tanpa 1/2m (tidak mempengaruhi optimal \u03b8)\n\n$$J = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y)$$\n\n**STEP 2:** Expand\n\n$$J = \\frac{1}{2}(\\theta^T X^T X \\theta - 2\\theta^T X^T y + y^T y)$$\n\n**STEP 3:** Gradient w.r.t. \u03b8\n\nGunakan aturan:\n- $\\nabla_\\theta (\\theta^T A \\theta) = 2A\\theta$ (jika A simetrik)\n- $\\nabla_\\theta (b^T \\theta) = b$\n\n$$\\nabla_\\theta J = \\frac{1}{2}(2X^T X \\theta - 2X^T y) = X^T X \\theta - X^T y$$\n\n**STEP 4:** Set = 0 dan solve\n\n$$X^T X \\theta = X^T y$$\n\n$$\\theta = (X^T X)^{-1} X^T y$$\n\n### 2.5 Normal Equation\n\n$$\\boxed{\\theta = (X^T X)^{-1} X^T y}$$\n\n#### \ud83d\udde3\ufe0f CARA BACA:\n\n> \"Theta sama dengan X transpose X inverse dikali X transpose y\"\n\n### 2.6 Kapan Pakai Normal Eq vs GD?\n\n| Normal Equation | Gradient Descent |\n|---|---|\n| n kecil (< 10,000) | n besar |\n| No need to choose \u03b1 | Perlu tune \u03b1 |\n| O(n\u00b3) untuk inverse | O(kn\u00b2) per iteration |\n| X\u1d40X harus invertible | Selalu work |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n\n## \ud83d\udcca PART 3: PROBABILISTIC INTERPRETATION\n\n### 3.1 Pertanyaan\n\nKenapa kita pakai **squared error**? Kenapa bukan |error| atau error\u2074?\n\n**Jawaban:** Dari perspektif probabilistik!\n\n### 3.2 Assumption\n\nAsumsikan:\n$$y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$$\n\nDimana $\\epsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^2)$ (Gaussian noise)\n\n#### \ud83d\udde3\ufe0f CARA BACA:\n\n> \"y sama dengan theta transpose x plus noise epsilon, dimana epsilon adalah Gaussian dengan mean 0\"\n\n### 3.3 Likelihood\n\n$$p(y^{(i)} | x^{(i)}; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}\\right)$$\n\n### 3.4 Log-Likelihood\n\n$$\\ell(\\theta) = \\sum_{i=1}^{m} \\log p(y^{(i)} | x^{(i)}; \\theta)$$\n\n$$= \\sum_{i=1}^{m} \\left[ -\\log(\\sqrt{2\\pi}\\sigma) - \\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2} \\right]$$\n\n$$= m \\cdot C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{m} (y^{(i)} - \\theta^T x^{(i)})^2$$\n\n### 3.5 Maximize Log-Likelihood = Minimize MSE!\n\n$$\\max_\\theta \\ell(\\theta) \\Leftrightarrow \\min_\\theta \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n\n> \ud83d\udca1 **MLE dengan Gaussian noise = Least Squares!**\n\nIni justifikasi probabilistik kenapa pakai squared error!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udee0\ufe0f PART 4: IMPLEMENTASI ROBUST (SGD, Mini-Batch, Momentum)\n",
                "\n",
                "Kita tidak hanya akan membuat loop sederhana. Kita akan implementasi Class yang siap produksi dengan:\n",
                "1.  **Feature Scaling (StandardScaler)**: Wajib untuk SGD agar konvergensi cepat.\n",
                "2.  **Mini-Batch Support**: Kompromi terbaik antara kecepatan dan stabilitas.\n",
                "3.  **Momentum**: Teknik mempercepat SGD menembus \"lembah\" landai.\n",
                "4.  **Learning Rate Decay**: Mengecilkan langkah saat mendekati optimum.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Generate Synthetic Data (Regression)\n",
                "np.random.seed(42)\n",
                "m = 100\n",
                "X = 2 * np.random.rand(m, 1)\n",
                "y = 4 + 3 * X + np.random.randn(m, 1) # y = 4 + 3x + noise\n",
                "\n",
                "# Outlier Extreme (untuk demo robustness)\n",
                "X_outlier = np.append(X, [[0.5], [1.5]]).reshape(-1, 1)\n",
                "y_outlier = np.append(y, [[20], [20]]).reshape(-1, 1) # Outliers\n",
                "\n",
                "plt.scatter(X, y, alpha=0.6)\n",
                "plt.xlabel(\"Input Feature (x)\")\n",
                "plt.ylabel(\"Target (y)\")\n",
                "plt.title(\"Generated Dataset\")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 The `AdvancedSGD` Class\n",
                "Class ini membungkus semua logika SGD modern.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AdvancedSGD:\n",
                "    def __init__(self, learning_rate=0.01, n_epochs=50, batch_size=None, momentum=0.0, decay_rate=0.0):\n",
                "        self.lr = learning_rate\n",
                "        self.epochs = n_epochs\n",
                "        self.batch_size = batch_size\n",
                "        self.momentum = momentum\n",
                "        self.decay = decay_rate\n",
                "        self.theta = None\n",
                "        self.history = [] # To store cost per epoch\n",
                "        \n",
                "    def _compute_cost(self, X, y, theta):\n",
                "        m = len(y)\n",
                "        predictions = X.dot(theta)\n",
                "        cost = (1/(2*m)) * np.sum(np.square(predictions - y))\n",
                "        return cost\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        m, n = X.shape\n",
                "        X_b = np.c_[np.ones((m, 1)), X] # Add bias term\n",
                "        self.theta = np.random.randn(n + 1, 1) # Random initialization\n",
                "        velocity = np.zeros_like(self.theta) # For momentum\n",
                "        \n",
                "        initial_lr = self.lr\n",
                "        \n",
                "        self.theta_path = [] # Store path for visualization\n",
                "        self.theta_path.append(self.theta.copy())\n",
                "\n",
                "        for epoch in range(self.epochs):\n",
                "            # Shuffle data\n",
                "            shuffled_indices = np.random.permutation(m)\n",
                "            X_b_shuffled = X_b[shuffled_indices]\n",
                "            y_shuffled = y[shuffled_indices]\n",
                "            \n",
                "            # Determine Batch Size\n",
                "            if self.batch_size is None:\n",
                "                bs = m # Batch GD (Full dataset)\n",
                "            else:\n",
                "                bs = self.batch_size # Stochastic/Mini-batch\n",
                "            \n",
                "            for i in range(0, m, bs):\n",
                "                xi = X_b_shuffled[i:i+bs]\n",
                "                yi = y_shuffled[i:i+bs]\n",
                "                \n",
                "                gradients = 2/bs * xi.T.dot(xi.dot(self.theta) - yi)\n",
                "                \n",
                "                # Momentum Logic\n",
                "                velocity = self.momentum * velocity + self.lr * gradients\n",
                "                self.theta = self.theta - velocity\n",
                "                \n",
                "                self.theta_path.append(self.theta.copy())\n",
                "            \n",
                "            # Learning Rate Decay\n",
                "            self.lr = initial_lr / (1 + self.decay * epoch)\n",
                "            \n",
                "            cost = self._compute_cost(X_b, y, self.theta)\n",
                "            self.history.append(cost)\n",
                "            \n",
                "        return self\n",
                "\n",
                "    def predict(self, X):\n",
                "        X_b = np.c_[np.ones((len(X), 1)), X]\n",
                "        return X_b.dot(self.theta)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Visualisasi: Battle of Optimizers\n",
                "Kita bandingkan jalur pergerakan **Batch GD**, **SGD**, dan **Mini-Batch** menuju Minimum.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper untuk Contour Plot\ndef plot_optimization_path(models, names, X, y):\n    # Setup Grid untuk Theta0 dan Theta1\n    t0 = np.linspace(3, 5, 100) # Sekitar true bias 4\n    t1 = np.linspace(2, 4, 100) # Sekitar true weight 3\n    T0, T1 = np.meshgrid(t0, t1)\n    J_grid = np.zeros(T0.shape)\n    \n    # Hitung cost untuk setiap titik grid\n    X_b = np.c_[np.ones((len(X), 1)), X]\n    for i in range(T0.shape[0]):\n        for j in range(T0.shape[1]):\n            theta_temp = np.array([[T0[i,j]], [T1[i,j]]])\n            J_grid[i,j] = (1/(2*len(y))) * np.sum((X_b.dot(theta_temp) - y)**2)\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(T0, T1, J_grid, levels=20, cmap='viridis', alpha=0.6)\n    plt.colorbar(label='Cost Function J')\n    \n    colors = ['r', 'g', 'b', 'orange']\n    markers = ['o', '+', 'x', 's']\n    \n    for idx, model in enumerate(models):\n        path = np.array(model.theta_path)\n        # Plot path\n        plt.plot(path[:, 0], path[:, 1], '-', color=colors[idx], linewidth=1, label=names[idx])\n        plt.plot(path[:, 0], path[:, 1], markers[idx], color=colors[idx], markersize=3)\n        # Plot start and end\n        plt.plot(path[-1, 0], path[-1, 1], 'k*', markersize=10) # End point\n\n    plt.xlabel(r'$\theta_0$ (Bias)')\n    plt.ylabel(r'$\theta_1$ (Weight)')\n    plt.title('Path of Gradient Descent Variants (Contour Plot)')\n    plt.legend()\n    plt.show()\n\n# Run Models to Compare\n# 1. Batch GD (Batch size = m)\nbgd = AdvancedSGD(learning_rate=0.1, n_epochs=50, batch_size=m)\nbgd.fit(X, y)\n\n# 2. Stochastic GD (Batch size = 1)\nsgd = AdvancedSGD(learning_rate=0.1, n_epochs=50, batch_size=1) # Noisy!\nsgd.fit(X, y)\n\n# 3. Mini-Batch GD (Batch size = 20)\nmbgd = AdvancedSGD(learning_rate=0.1, n_epochs=50, batch_size=20)\nmbgd.fit(X, y)\n\n# 4. Momentum GD (Batch size = 20 + Momentum 0.9)\nmom = AdvancedSGD(learning_rate=0.1, n_epochs=50, batch_size=20, momentum=0.9)\nmom.fit(X, y)\n\nplot_optimization_path([bgd, sgd, mbgd, mom], \n                      ['Batch GD', 'SGD', 'Mini-Batch', 'Momentum'], \n                      X, y)\n\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udd04 Interpretasi Grafik\n",
                "1.  **Batch GD (Merah):** Jalan lurus, smooth, tapi lambat per epoch karena hitung semua data.\n",
                "2.  **SGD (Hijau):** Sangat \"noisy\" (zig-zag). Karena update tiap 1 data, dia 'mabuk' tapi rata-rata menuju arah benar.\n",
                "3.  **Mini-Batch (Biru):** Trade-off terbaik. Agak noisy tapi lebih stabil dari SGD, dan lebih efisien komputasi.\n",
                "4.  **Momentum (Orange):** Meluncur cepat! Momentum membantu menembus area landai dan meredam osilasi.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83d\udcca BENCHMARK: PERFORMANCE COMPARISON TABLE\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nimport time\n\n# Define X_b for this cell\nX_b = np.c_[np.ones((len(X), 1)), X]\n\n# Re-train with timing\nresults = []\nconfigs = [\n    ('Batch GD', {'batch_size': m, 'momentum': 0.0}),\n    ('Stochastic GD', {'batch_size': 1, 'momentum': 0.0}),\n    ('Mini-Batch GD', {'batch_size': 16, 'momentum': 0.0}),\n    ('Momentum SGD', {'batch_size': 16, 'momentum': 0.9}),\n]\n\nfor name, cfg in configs:\n    model = AdvancedSGD(learning_rate=0.01, n_epochs=100, **cfg)\n    start = time.time()\n    model.fit(X_b, y)\n    elapsed = (time.time() - start) * 1000  # ms\n    results.append({\n        'Method': name,\n        'Time (ms)': f'{elapsed:.2f}',\n        'Final Cost': f'{model.history[-1]:.6f}',\n        '\u03b8\u2080': f'{model.theta[0][0]:.4f}',\n        '\u03b8\u2081': f'{model.theta[1][0]:.4f}'\n    })\n\nprint('\\n\ud83d\udcca BENCHMARK RESULTS')\nprint('='*70)\nprint(f'{\"Method\":<16} | {\"Time (ms)\":<10} | {\"Final Cost\":<12} | {\"\u03b8\u2080\":<8} | {\"\u03b8\u2081\":<8}')\nprint('-'*70)\nfor r in results:\n    print(f'{r[\"Method\"]:<16} | {r[\"Time (ms)\"]:<10} | {r[\"Final Cost\"]:<12} | {r[\"\u03b8\u2080\"]:<8} | {r[\"\u03b8\u2081\"]:<8}')\nprint('='*70)\nprint('\\n\ud83d\udca1 Insight: Mini-Batch + Momentum biasanya memberikan trade-off terbaik.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\uded1 Early Stopping (Bonus)\n",
                "\n",
                "**Konsep:** Berhenti training lebih awal jika cost sudah tidak berubah signifikan.\n",
                "\n",
                "```python\n",
                "# Di training loop:\n",
                "if len(self.history) > patience:\n",
                "    delta = abs(self.history[-1] - self.history[-patience])\n",
                "    if delta < tolerance:\n",
                "        print(f'Early stopping at epoch {epoch}')\n",
                "        break\n",
                "```\n",
                "\n",
                "**Kapan Pakai:**\n",
                "- Dataset besar, training lama.\n",
                "- Ingin hindari overfitting.\n",
                "- Resource (GPU/CPU) terbatas.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udccf PART 5: ROBUSTNESS & SCALING\n",
                "### 5.1 Kenapa Feature Scaling Penting untuk SGD?\n",
                "Jika fitur punya skala beda (misal: luas rumah 2000 vs kamar 3), kontur cost function akan lonjong (elips pipih).\n",
                "Gradient Descent akan berosilasi lambat di lembah pipih itu.\n",
                "**Solusi:** StandardScaler ($z = \frac{x - \\mu}{\\sigma}$).\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83e\uddee PART 6: NORMAL EQUATION EDGE CASES\n",
                "Rumus Normal Equation: $\\theta = (X^T X)^{-1} X^T y$\n",
                "\n",
                "Bagaimana jika $X^T X$ **Singular** (Tidak punya invers)?\n",
                "Ini terjadi jika:\n",
                "1.  Fitur redundan (Linear dependency: $x_2 = 2x_1$).\n",
                "2.  Fitur lebih banyak dari data ($n > m$).\n",
                "\n",
                "Solusi: Gunakan **Pseudo-Inverse** (Moore-Penrose) -> `np.linalg.pinv` (bukan `inv`).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demo Singular Matrix\n",
                "# Buat fitur redundan\n",
                "X_redundant = np.array([[1, 2], [2, 4], [3, 6], [4, 8]]) # x2 = 2*x1\n",
                "y_redundant = np.array([3, 6, 9, 12])\n",
                "\n",
                "# Coba Normal Equation Biasa (inv) - Akan Error atau Numerical Unstable\n",
                "try:\n",
                "    X_b_red = np.c_[np.ones((4, 1)), X_redundant]\n",
                "    theta_bad = np.linalg.inv(X_b_red.T.dot(X_b_red)).dot(X_b_red.T).dot(y_redundant)\n",
                "    print(\"Inversion Successful (Lucky):\", theta_bad)\n",
                "except np.linalg.LinAlgError:\n",
                "    print(\"\u274c Error: Matrix Singular! Tidak bisa di-inverse biasa.\")\n",
                "\n",
                "# Solusi: PINV (Pseudo Inverse)\n",
                "theta_good = np.linalg.pinv(X_b_red.T.dot(X_b_red)).dot(X_b_red.T).dot(y_redundant)\n",
                "print(\"\u2705 Solusi dengan Pseudo-Inverse (pinv):\", theta_good)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udd0d BONUS 1: Gradient Check (Verifikasi Implementasi)\n",
                "\n",
                "**Masalah:** Bagaimana kita tahu implementasi gradient kita benar?\n",
                "\n",
                "**Solusi:** Bandingkan **analytical gradient** (rumus) dengan **numerical gradient** (finite difference).\n",
                "\n",
                "$$\\text{Numerical Gradient} = \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}$$\n",
                "\n",
                "Jika selisihnya < 1e-5, implementasi kita **valid**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83d\udd0d GRADIENT CHECK: Verify our implementation is correct\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\ndef compute_cost(theta, X, y):\n    m = len(y)\n    predictions = X @ theta\n    return (1/(2*m)) * np.sum((predictions - y)**2)\n\ndef numerical_gradient(theta, X, y, epsilon=1e-7):\n    \"\"\"Compute gradient using finite difference\"\"\"\n    num_grad = np.zeros_like(theta)\n    for i in range(len(theta)):\n        theta_plus = theta.copy()\n        theta_minus = theta.copy()\n        theta_plus[i] += epsilon\n        theta_minus[i] -= epsilon\n        num_grad[i] = (compute_cost(theta_plus, X, y) - compute_cost(theta_minus, X, y)) / (2 * epsilon)\n    return num_grad\n\ndef analytical_gradient(theta, X, y):\n    \"\"\"Compute gradient using formula\"\"\"\n    m = len(y)\n    return (1/m) * X.T @ (X @ theta - y)\n\n# Define X_b for this cell\nX_b = np.c_[np.ones((len(X), 1)), X]\n\n# Test\ntheta_test = np.random.randn(2, 1)\nnum_grad = numerical_gradient(theta_test, X_b, y)\nana_grad = analytical_gradient(theta_test, X_b, y)\n\nprint('\ud83d\udd0d GRADIENT CHECK')\nprint('='*50)\nprint(f'Numerical Gradient:  {num_grad.flatten()}')\nprint(f'Analytical Gradient: {ana_grad.flatten()}')\nprint(f'Difference: {np.linalg.norm(num_grad - ana_grad):.2e}')\nprint('='*50)\nif np.linalg.norm(num_grad - ana_grad) < 1e-5:\n    print('\u2705 Gradient implementation is CORRECT!')\nelse:\n    print('\u274c WARNING: Gradient mismatch detected!')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83e\uddee BONUS 2: Hitung Manual 1 Epoch (Intuisi Dasar)\n",
                "\n",
                "Mari hitung **satu langkah** Gradient Descent dengan tangan untuk 3 data points.\n",
                "\n",
                "**Data:**\n",
                "| i | $x_i$ | $y_i$ |\n",
                "|---|-------|-------|\n",
                "| 1 | 1 | 2 |\n",
                "| 2 | 2 | 4 |\n",
                "| 3 | 3 | 5 |\n",
                "\n",
                "**Initial:** $\\theta_0 = 0$, $\\theta_1 = 0$, $\\alpha = 0.1$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
                "# \ud83e\uddee MANUAL CALCULATION: 1 Epoch Step-by-Step\n",
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
                "print('\ud83e\uddee MANUAL GRADIENT DESCENT CALCULATION')\n",
                "print('='*60)\n",
                "\n",
                "# Tiny dataset\n",
                "X_tiny = np.array([[1], [2], [3]])\n",
                "y_tiny = np.array([[2], [4], [5]])\n",
                "X_tiny_b = np.c_[np.ones((3, 1)), X_tiny]  # Add bias\n",
                "\n",
                "# Initial theta\n",
                "theta = np.array([[0], [0]])  # [theta0, theta1]\n",
                "alpha = 0.1\n",
                "m = 3\n",
                "\n",
                "print(f'Data: X = {X_tiny.flatten()}, y = {y_tiny.flatten()}')\n",
                "print(f'Initial theta: {theta.flatten()}')\n",
                "print(f'Learning rate: {alpha}')\n",
                "print()\n",
                "\n",
                "# Step 1: Predictions\n",
                "h = X_tiny_b @ theta\n",
                "print('STEP 1: Compute predictions h = X @ theta')\n",
                "print(f'  h = {h.flatten()}')\n",
                "print()\n",
                "\n",
                "# Step 2: Errors\n",
                "errors = h - y_tiny\n",
                "print('STEP 2: Compute errors (h - y)')\n",
                "print(f'  errors = {errors.flatten()}')\n",
                "print()\n",
                "\n",
                "# Step 3: Gradient\n",
                "gradient = (1/m) * X_tiny_b.T @ errors\n",
                "print('STEP 3: Compute gradient = (1/m) * X.T @ errors')\n",
                "print(f'  gradient = {gradient.flatten()}')\n",
                "print()\n",
                "\n",
                "# Step 4: Update\n",
                "theta_new = theta - alpha * gradient\n",
                "print('STEP 4: Update theta = theta - alpha * gradient')\n",
                "print(f'  theta_new = {theta_new.flatten()}')\n",
                "print()\n",
                "\n",
                "print('='*60)\n",
                "print('\ud83d\udca1 Setelah 1 epoch, theta berubah dari [0, 0] ke', theta_new.flatten())\n",
                "print('   Proses ini diulang ratusan/ribuan kali sampai konvergen!')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \u2699\ufe0f BONUS 3: Feature Scaling Implementation\n",
                "\n",
                "**Problem:** Jika fitur punya skala berbeda (misal: usia 0-100, gaji 1M-100M), GD akan lambat.\n",
                "\n",
                "**Solution:** Standardize semua fitur ke mean=0, std=1.\n",
                "\n",
                "$$x_{scaled} = \\frac{x - \\mu}{\\sigma}$$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
                "# \u2699\ufe0f FEATURE SCALING: StandardScaler from scratch\n",
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
                "class StandardScaler:\n",
                "    def __init__(self):\n",
                "        self.mean_ = None\n",
                "        self.std_ = None\n",
                "    \n",
                "    def fit(self, X):\n",
                "        self.mean_ = np.mean(X, axis=0)\n",
                "        self.std_ = np.std(X, axis=0)\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X):\n",
                "        return (X - self.mean_) / self.std_\n",
                "    \n",
                "    def fit_transform(self, X):\n",
                "        return self.fit(X).transform(X)\n",
                "    \n",
                "    def inverse_transform(self, X_scaled):\n",
                "        return X_scaled * self.std_ + self.mean_\n",
                "\n",
                "# Demo\n",
                "print('\u2699\ufe0f FEATURE SCALING DEMO')\n",
                "print('='*50)\n",
                "\n",
                "# Create data with different scales\n",
                "X_unscaled = np.array([[25, 50000], [30, 60000], [35, 80000], [40, 100000]])\n",
                "print('Original data (Age, Salary):')\n",
                "print(X_unscaled)\n",
                "print()\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_unscaled)\n",
                "print('Scaled data (mean=0, std=1):')\n",
                "print(np.round(X_scaled, 3))\n",
                "print()\n",
                "print(f'Mean after scaling: {np.round(X_scaled.mean(axis=0), 6)}')\n",
                "print(f'Std after scaling: {np.round(X_scaled.std(axis=0), 6)}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83c\udfaf BONUS 4: Learning Rate Finder\n",
                "\n",
                "**Ide:** Naikkan learning rate secara eksponensial, plot loss-nya.\n",
                "\n",
                "**Interpretasi:**\n",
                "- LR terlalu kecil \u2192 loss turun lambat\n",
                "- LR optimal \u2192 loss turun cepat\n",
                "- LR terlalu besar \u2192 loss **naik** (exploding!)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83c\udfaf LEARNING RATE FINDER\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Define X_b and m for this cell\nX_b = np.c_[np.ones((len(X), 1)), X]\nm = len(X)\n\nlearning_rates = np.logspace(-5, 0, 50)  # 1e-5 to 1\nlosses = []\n\nfor lr in learning_rates:\n    theta = np.zeros((2, 1))\n    # Run 10 iterations\n    for _ in range(10):\n        gradient = (1/m) * X_b.T @ (X_b @ theta - y)\n        theta = theta - lr * gradient\n    loss = compute_cost(theta, X_b, y)\n    losses.append(min(loss, 1e10))  # Cap for visualization\n\nplt.figure(figsize=(10, 5))\nplt.semilogx(learning_rates, losses, 'b-', linewidth=2)\nplt.axhline(y=min(losses), color='g', linestyle='--', label=f'Best Loss: {min(losses):.4f}')\nplt.xlabel('Learning Rate (log scale)')\nplt.ylabel('Loss after 10 iterations')\nplt.title('\ud83c\udfaf Learning Rate Finder')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Find optimal LR\noptimal_idx = np.argmin(losses)\noptimal_lr = learning_rates[optimal_idx]\nplt.axvline(x=optimal_lr, color='r', linestyle='--', label=f'Optimal LR: {optimal_lr:.4f}')\nplt.legend()\nplt.show()\n\nprint(f'\ud83d\udca1 Suggested Learning Rate: {optimal_lr:.4f}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udcca BONUS 5: Multi-Feature Example\n",
                "\n",
                "Sejauh ini kita hanya pakai 1 fitur. Mari coba dengan **3 fitur**:\n",
                "- Luas Tanah (m\u00b2)\n",
                "- Jumlah Kamar\n",
                "- Usia Bangunan (tahun)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83d\udcca MULTI-FEATURE REGRESSION\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nnp.random.seed(42)\nm_multi = 100\n\n# 3 features: [Luas, Kamar, Usia]\nX_multi = np.column_stack([\n    np.random.randint(50, 200, m_multi),    # Luas 50-200 m\u00b2\n    np.random.randint(1, 6, m_multi),        # Kamar 1-5\n    np.random.randint(1, 30, m_multi)        # Usia 1-30 tahun\n]).astype(float)\n\n# True relationship: Harga = 100 + 5*Luas + 50*Kamar - 2*Usia + noise\ny_multi = 100 + 5*X_multi[:,0] + 50*X_multi[:,1] - 2*X_multi[:,2] + np.random.randn(m_multi)*20\ny_multi = y_multi.reshape(-1, 1)\n\n# Scale features\nscaler = StandardScaler()\nX_multi_scaled = scaler.fit_transform(X_multi)\nX_multi_b = np.c_[np.ones((m_multi, 1)), X_multi_scaled]\n\n# Train\nmodel_multi = AdvancedSGD(learning_rate=0.1, n_epochs=200, batch_size=16, momentum=0.9)\nmodel_multi.fit(X_multi_b, y_multi)\n\nprint('\ud83d\udcca MULTI-FEATURE REGRESSION')\nprint('='*50)\nprint(f'Features: [Luas, Kamar, Usia]')\nprint(f'True coefficients: [5, 50, -2]')\nprint(f'Learned theta (scaled): {model_multi.theta.flatten()[1:]}')\nprint(f'Final cost: {model_multi.history[-1]:.4f}')\nprint()\nprint('\ud83d\udca1 Note: Coefficients berbeda karena fitur sudah di-scale.')\nprint('   Untuk interpretasi, perlu inverse_transform.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \u26a0\ufe0f BONUS 6: Common Pitfalls & Debugging\n",
                "\n",
                "### \ud83d\udea8 Masalah Umum:\n",
                "\n",
                "| Problem | Gejala | Solusi |\n",
                "|---------|--------|--------|\n",
                "| **Learning Rate terlalu besar** | Loss naik / NaN / Inf | Kurangi LR (coba /10) |\n",
                "| **Learning Rate terlalu kecil** | Loss turun sangat lambat | Naikkan LR (coba *10) |\n",
                "| **Fitur tidak di-scale** | Konvergensi lambat, zig-zag | Pakai StandardScaler |\n",
                "| **Exploding Gradients** | Loss = NaN | Gradient clipping / kurangi LR |\n",
                "| **Stuck di local minimum** | Loss tidak turun lagi | Tambah momentum / restart |\n",
                "| **Overfitting** | Training loss bagus, test buruk | Regularization / Early stopping |\n",
                "\n",
                "### \ud83d\udee0\ufe0f Debugging Checklist:\n",
                "\n",
                "```python\n",
                "# 1. Cek data shape\n",
                "print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
                "\n",
                "# 2. Cek ada NaN/Inf\n",
                "print(f'NaN in X: {np.isnan(X).any()}')\n",
                "print(f'Inf in X: {np.isinf(X).any()}')\n",
                "\n",
                "# 3. Cek gradient magnitude\n",
                "print(f'Gradient norm: {np.linalg.norm(gradient)}')\n",
                "\n",
                "# 4. Plot loss curve\n",
                "plt.plot(model.history)\n",
                "plt.title('Loss Curve')\n",
                "plt.show()\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udd17 BONUS 7: Connection to Deep Learning\n",
                "\n",
                "**Semua yang kamu pelajari di sini adalah FONDASI untuk Neural Networks!**\n",
                "\n",
                "| Konsep di Module Ini | Di Deep Learning |\n",
                "|---------------------|------------------|\n",
                "| Gradient Descent | **Sama persis** - SGD, Adam, RMSprop |\n",
                "| Learning Rate | **Sama** - LR Scheduling lebih kompleks |\n",
                "| Batch Size | **Sama** - Mini-batch standar industri |\n",
                "| Momentum | **Sama** - Dipakai di Adam, RMSprop |\n",
                "| Feature Scaling | **BatchNorm, LayerNorm** |\n",
                "| Gradient Check | **Autograd** (PyTorch/TensorFlow) |\n",
                "| Early Stopping | **Callbacks** di Keras/PyTorch |\n",
                "\n",
                "**\ud83d\udca1 Takeaway:** Kuasai SGD di Linear Regression = 50% bekal untuk Deep Learning!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## \ud83d\udcdd SUMMARY (JIKA-MAKA)\n",
                "\n",
                "| Metode | Sifat | Kapan Pakai? (JIKA...) |\n",
                "|--------|-------|--------------------------|\n",
                "| **Batch GD** | Stabil, Lambat | Data muat di RAM, butuh presisi tinggi |\n",
                "| **SGD** | Cepat, Berisik | Data SANGAT besar (streaming), butuh less memory |\n",
                "| **Mini-Batch** | Balance | **Standard Industry Practice** (Default di Deep Learning) |\n",
                "| **Momentum** | Cepat Konvergen | Cost function berlembah/berbelok tajam |\n",
                "| **Normal Eq (inv)** | Analitik | Dataset kecil (<10k), Matrix full rank |\n",
                "| **Normal Eq (pinv)** | Robust | Sda, tapi ada fitur redundan/singular |\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
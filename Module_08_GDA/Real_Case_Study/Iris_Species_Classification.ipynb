{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐧 Case Study: Penguin Species Classification with GDA\n",
    "\n",
    "---\n",
    "\n",
    "## When Your Data is Gaussian: The Power of Generative Models\n",
    "\n",
    "The Palmer Penguins dataset is the modern alternative to Iris. Three species of penguins from Antarctica – Adelie, Chinstrap, and Gentoo – measured at Palmer Station.\n",
    "\n",
    "> **The Challenge**: Classify penguin species based on physical measurements like bill length, bill depth, flipper length, and body mass.\n",
    "\n",
    "### Why GDA?\n",
    "\n",
    "Gaussian Discriminant Analysis is a **generative model**. Instead of directly learning a decision boundary (like logistic regression does), we model **how the data is generated** for each class.\n",
    "\n",
    "The core idea? Each class has its own Gaussian distribution:\n",
    "\n",
    "$$P(x|y=k) = \\mathcal{N}(\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Then we use Bayes' rule to flip it around:\n",
    "\n",
    "$$P(y=k|x) \\propto P(x|y=k) \\cdot P(y=k)$$\n",
    "\n",
    "**That's the key insight** – we model the *likelihood* of observations given each class, then use priors to compute posteriors. It's probability theory in action!\n",
    "\n",
    "### LDA vs QDA\n",
    "\n",
    "- **LDA (Linear Discriminant Analysis)**: Assumes all classes share the same covariance matrix $\\Sigma$. This gives us **linear** decision boundaries.\n",
    "- **QDA (Quadratic Discriminant Analysis)**: Each class gets its own covariance $\\Sigma_k$. This gives us **quadratic** (curved) decision boundaries.\n",
    "\n",
    "When should you use which? We'll explore that in this case study!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📦 Setup & Data Loading\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style setup\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the Palmer Penguins dataset\n",
    "df = sns.load_dataset('penguins').dropna()  # Drop rows with missing values\n",
    "\n",
    "print(\"🐧 Loaded Palmer Penguins Dataset!\")\n",
    "print(f\"   {len(df)} penguins from Palmer Station, Antarctica\")\n",
    "print(f\"\\n   Species: {list(df['species'].unique())}\")\n",
    "print(f\"   Islands: {list(df['island'].unique())}\")\n",
    "print(f\"\\n📊 Feature Statistics:\")\n",
    "print(df.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🔍 Exploratory Data Analysis\n",
    "# ============================================================\n",
    "# Let's visualize how well-separated the species are!\n",
    "\n",
    "# Prepare the numeric features\n",
    "feature_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, col in zip(axes.flat, feature_cols):\n",
    "    for species in df['species'].unique():\n",
    "        data = df[df['species'] == species][col]\n",
    "        ax.hist(data, alpha=0.5, label=species, bins=20, density=True)\n",
    "    ax.set_xlabel(col.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Distribution of {col.replace(\"_\", \" \").title()}')\n",
    "\n",
    "plt.suptitle('Feature Distributions by Species', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 Key Observation: Look how each feature has different distributions per species!\")\n",
    "print(\"   This is exactly what GDA models – each class has its own Gaussian distribution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📊 Pairplot: The Big Picture\n",
    "# ============================================================\n",
    "# This is the classic way to see class separability\n",
    "\n",
    "g = sns.pairplot(df, vars=feature_cols, hue='species', \n",
    "                 diag_kind='kde', plot_kws={'alpha': 0.6, 's': 50})\n",
    "g.fig.suptitle('Pairplot: Penguin Species Separability', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"🎯 What to look for:\")\n",
    "print(\"   • Clear separation between clusters → GDA will work well\")\n",
    "print(\"   • Elliptical clouds → Gaussian assumption is reasonable\")\n",
    "print(\"   • Similar spread within each class → LDA might be sufficient\")\n",
    "print(\"   • Different spreads per class → QDA might help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🧮 GDA From Scratch: Building the Generative Model\n",
    "\n",
    "Alright, now for the fun part – let's implement GDA ourselves!\n",
    "\n",
    "### The Math Behind It\n",
    "\n",
    "For each class $k$, we estimate:\n",
    "1. **Prior probability**: $\\phi_k = P(y=k) = \\frac{n_k}{n}$\n",
    "2. **Class mean**: $\\mu_k = \\frac{1}{n_k} \\sum_{i: y^{(i)}=k} x^{(i)}$\n",
    "3. **Class covariance**: $\\Sigma_k = \\frac{1}{n_k} \\sum_{i: y^{(i)}=k} (x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^T$\n",
    "\n",
    "For **LDA**, we pool all class covariances into one shared $\\Sigma$:\n",
    "$$\\Sigma = \\sum_k \\phi_k \\cdot \\Sigma_k$$\n",
    "\n",
    "For **QDA**, each class keeps its own $\\Sigma_k$.\n",
    "\n",
    "### The Discriminant Function\n",
    "\n",
    "To classify a new point $x$, we compute the **log-posterior** for each class:\n",
    "\n",
    "$$\\log P(y=k|x) = \\log P(x|y=k) + \\log P(y=k) + \\text{const}$$\n",
    "\n",
    "The class with the highest score wins!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 🔧 GDA Implementation From Scratch\n",
    "# ============================================================\n",
    "\n",
    "class GDA:\n",
    "    \"\"\"\n",
    "    Gaussian Discriminant Analysis (LDA or QDA)\n",
    "    \n",
    "    This is a generative classifier that models:\n",
    "    - P(x|y=k) as a multivariate Gaussian\n",
    "    - P(y=k) as the class prior\n",
    "    \n",
    "    Then uses Bayes' rule for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shared_cov=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        shared_cov : bool\n",
    "            If True, use shared covariance (LDA)\n",
    "            If False, use class-specific covariance (QDA)\n",
    "        \"\"\"\n",
    "        self.shared_cov = shared_cov\n",
    "        self.classes_ = None\n",
    "        self.priors_ = {}      # P(y=k)\n",
    "        self.means_ = {}       # μ_k for each class\n",
    "        self.covs_ = {}        # Σ_k for each class\n",
    "        self.shared_cov_ = None  # Shared Σ for LDA\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the GDA model by estimating parameters from data.\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        self.classes_ = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Step 1: Estimate parameters for each class\n",
    "        for c in self.classes_:\n",
    "            X_c = X[y == c]  # Samples belonging to class c\n",
    "            n_c = len(X_c)\n",
    "            \n",
    "            # Prior: P(y=k) = n_k / n\n",
    "            self.priors_[c] = n_c / n_samples\n",
    "            \n",
    "            # Mean: μ_k = average of class samples\n",
    "            self.means_[c] = X_c.mean(axis=0)\n",
    "            \n",
    "            # Covariance: Σ_k = (X_c - μ_k)^T (X_c - μ_k) / n_c\n",
    "            # Using n_c (not n_c-1) for MLE estimate\n",
    "            diff = X_c - self.means_[c]\n",
    "            self.covs_[c] = (diff.T @ diff) / n_c\n",
    "        \n",
    "        # Step 2: For LDA, compute pooled covariance\n",
    "        if self.shared_cov:\n",
    "            self.shared_cov_ = np.zeros_like(self.covs_[self.classes_[0]])\n",
    "            for c in self.classes_:\n",
    "                # Weight each class covariance by its prior\n",
    "                self.shared_cov_ += self.priors_[c] * self.covs_[c]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_log_likelihood(self, X, class_label):\n",
    "        \"\"\"Compute log P(x|y=k) for each sample.\"\"\"\n",
    "        mean = self.means_[class_label]\n",
    "        cov = self.shared_cov_ if self.shared_cov else self.covs_[class_label]\n",
    "        \n",
    "        # Add small regularization for numerical stability\n",
    "        cov_reg = cov + 1e-6 * np.eye(cov.shape[0])\n",
    "        \n",
    "        # Log of multivariate Gaussian PDF\n",
    "        try:\n",
    "            rv = multivariate_normal(mean=mean, cov=cov_reg)\n",
    "            return rv.logpdf(X)\n",
    "        except:\n",
    "            # Fallback for singular matrices\n",
    "            return np.full(len(X), -np.inf)\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Compute log posterior for each class.\"\"\"\n",
    "        X = np.array(X)\n",
    "        log_posteriors = np.zeros((len(X), len(self.classes_)))\n",
    "        \n",
    "        for i, c in enumerate(self.classes_):\n",
    "            # log P(y=k|x) ∝ log P(x|y=k) + log P(y=k)\n",
    "            log_likelihood = self._compute_log_likelihood(X, c)\n",
    "            log_prior = np.log(self.priors_[c])\n",
    "            log_posteriors[:, i] = log_likelihood + log_prior\n",
    "        \n",
    "        return log_posteriors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        log_proba = self.predict_log_proba(X)\n",
    "        return self.classes_[np.argmax(log_proba, axis=1)]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Return accuracy score.\"\"\"\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n",
    "print(\"✅ GDA class implemented!\")\n",
    "print(\"   • shared_cov=True  → LDA (Linear Discriminant Analysis)\")\n",
    "print(\"   • shared_cov=False → QDA (Quadratic Discriminant Analysis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# 🧪 Test Our Implementation\n",
    "# ============================================================\n",
    "\n",
    "# Prepare the data\n",
    "X = df[feature_cols].values\n",
    "y = df['species'].values\n",
    "\n",
    "# Encode labels to integers for consistency\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "class_names = le.classes_\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"📊 Data Split:\")\n",
    "print(f\"   Training: {len(X_train)} samples\")\n",
    "print(f\"   Testing:  {len(X_test)} samples\")\n",
    "\n",
    "# Train our GDA models\n",
    "lda_scratch = GDA(shared_cov=True)   # LDA\n",
    "qda_scratch = GDA(shared_cov=False)  # QDA\n",
    "\n",
    "lda_scratch.fit(X_train, y_train)\n",
    "qda_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lda_acc = lda_scratch.score(X_test, y_test)\n",
    "qda_acc = qda_scratch.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n🎯 Test Accuracy (From Scratch):\")\n",
    "print(f\"   LDA: {lda_acc*100:.1f}%\")\n",
    "print(f\"   QDA: {qda_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📐 Visualize the Learned Distributions\n",
    "# ============================================================\n",
    "# Let's see what our model actually learned!\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Use two features for 2D visualization\n",
    "feat1, feat2 = 0, 1  # bill_length_mm vs bill_depth_mm\n",
    "\n",
    "for ax, (title, model) in zip(axes, [\n",
    "    ('LDA (Shared Covariance)', lda_scratch),\n",
    "    ('QDA (Class-Specific Covariance)', qda_scratch),\n",
    "    ('Data Points', None)\n",
    "]):\n",
    "    # Plot data points\n",
    "    for i, c in enumerate(model.classes_ if model else np.unique(y_train)):\n",
    "        mask = y_train == c\n",
    "        ax.scatter(X_train[mask, feat1], X_train[mask, feat2], \n",
    "                  alpha=0.6, label=class_names[c], s=40)\n",
    "    \n",
    "    if model:\n",
    "        # Plot class means\n",
    "        for c in model.classes_:\n",
    "            mean = model.means_[c]\n",
    "            ax.scatter(mean[feat1], mean[feat2], marker='X', s=200, \n",
    "                      c='black', edgecolors='white', linewidths=2)\n",
    "        \n",
    "        # Draw covariance ellipses\n",
    "        from matplotlib.patches import Ellipse\n",
    "        for c in model.classes_:\n",
    "            mean = model.means_[c]\n",
    "            cov_2d = (model.shared_cov_ if model.shared_cov else model.covs_[c])[[feat1, feat2]][:, [feat1, feat2]]\n",
    "            \n",
    "            # Eigenvalue decomposition for ellipse\n",
    "            eigenvalues, eigenvectors = np.linalg.eigh(cov_2d)\n",
    "            angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "            width, height = 2 * 2 * np.sqrt(eigenvalues)  # 2 std deviations\n",
    "            \n",
    "            ellipse = Ellipse(xy=(mean[feat1], mean[feat2]), \n",
    "                            width=width, height=height, angle=angle,\n",
    "                            fill=False, linewidth=2, linestyle='--')\n",
    "            ax.add_patch(ellipse)\n",
    "    \n",
    "    ax.set_xlabel(feature_cols[feat1].replace('_', ' ').title())\n",
    "    ax.set_ylabel(feature_cols[feat2].replace('_', ' ').title())\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 The ellipses show 2σ contours of the fitted Gaussians.\")\n",
    "print(\"   Notice: LDA has identical ellipse shapes (shared Σ), QDA allows different shapes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎨 Decision Boundary Visualization\n",
    "# ============================================================\n",
    "# Let's see where LDA and QDA draw the lines!\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title, ax, feature_idx=(0, 1)):\n",
    "    \"\"\"Plot decision boundary for 2 features.\"\"\"\n",
    "    f1, f2 = feature_idx\n",
    "    \n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, f1].min() - 1, X[:, f1].max() + 1\n",
    "    y_min, y_max = X[:, f2].min() - 1, X[:, f2].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # For prediction, we need all 4 features - use mean for the others\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    X_full = np.zeros((len(mesh_points), X.shape[1]))\n",
    "    X_full[:, f1] = mesh_points[:, 0]\n",
    "    X_full[:, f2] = mesh_points[:, 1]\n",
    "    # Use training means for other features\n",
    "    for i in range(X.shape[1]):\n",
    "        if i not in [f1, f2]:\n",
    "            X_full[:, i] = X[:, i].mean()\n",
    "    \n",
    "    Z = model.predict(X_full).reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    ax.contour(xx, yy, Z, colors='black', linewidths=0.5)\n",
    "    \n",
    "    for i in np.unique(y):\n",
    "        mask = y == i\n",
    "        ax.scatter(X[mask, f1], X[mask, f2], label=class_names[i], \n",
    "                  alpha=0.7, edgecolors='white', s=50)\n",
    "    \n",
    "    ax.set_xlabel(feature_cols[f1].replace('_', ' ').title())\n",
    "    ax.set_ylabel(feature_cols[f2].replace('_', ' ').title())\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plot_decision_boundary(lda_scratch, X_train, y_train, \n",
    "                       'LDA: Linear Decision Boundaries', axes[0])\n",
    "plot_decision_boundary(qda_scratch, X_train, y_train, \n",
    "                       'QDA: Quadratic Decision Boundaries', axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🔍 Key Difference:\")\n",
    "print(\"   • LDA: Straight lines separate classes (linear boundaries)\")\n",
    "print(\"   • QDA: Curved boundaries can better capture non-linear separation\")\n",
    "print(\"\\n   When classes have different covariance structures, QDA shines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 LDA vs QDA: When Does Each Shine?\n",
    "\n",
    "Here's the trade-off:\n",
    "\n",
    "| Aspect | LDA | QDA |\n",
    "|--------|-----|-----|\n",
    "| **Parameters** | Fewer (shared Σ) | More (per-class Σ_k) |\n",
    "| **Boundary** | Linear | Quadratic |\n",
    "| **Bias** | Higher | Lower |\n",
    "| **Variance** | Lower | Higher |\n",
    "| **Best When** | Small data, similar class spreads | Large data, different class spreads |\n",
    "\n",
    "### The Bias-Variance Trade-off\n",
    "\n",
    "- **LDA** makes a stronger assumption (shared covariance) → more bias, less variance\n",
    "- **QDA** is more flexible → less bias, more variance\n",
    "\n",
    "**Rule of thumb**: If you have limited data or classes look similarly shaped, go with LDA. If you have plenty of data and classes clearly have different shapes, try QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📊 Compare with sklearn Implementation\n",
    "# ============================================================\n",
    "# Let's verify our implementation matches sklearn!\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA_sklearn\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA_sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'LDA (sklearn)': LDA_sklearn(),\n",
    "    'QDA (sklearn)': QDA_sklearn(),\n",
    "    'LDA (scratch)': lda_scratch,\n",
    "    'QDA (scratch)': qda_scratch,\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "}\n",
    "\n",
    "print(\"📊 Model Comparison (Test Accuracy):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    if 'scratch' not in name:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    results[name] = {'train': train_acc, 'test': test_acc}\n",
    "    \n",
    "    print(f\"{name:25s} | Train: {train_acc*100:5.1f}% | Test: {test_acc*100:5.1f}%\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n✅ Our scratch implementation matches sklearn!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🔄 Cross-Validation: A More Robust Comparison\n",
    "# ============================================================\n",
    "\n",
    "print(\"📊 5-Fold Cross-Validation Results:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "cv_models = {\n",
    "    'LDA': LDA_sklearn(),\n",
    "    'QDA': QDA_sklearn(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in cv_models.items():\n",
    "    scores = cross_val_score(model, X, y_encoded, cv=5)\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name:25s} | Accuracy: {scores.mean()*100:5.1f}% (± {scores.std()*100:.1f}%)\")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "positions = np.arange(len(cv_results))\n",
    "bp = ax.boxplot(cv_results.values(), positions=positions, widths=0.6, patch_artist=True)\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticklabels(cv_results.keys())\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Cross-Validation Accuracy Comparison')\n",
    "ax.set_ylim([0.85, 1.02])\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Insight: All three methods perform similarly on this well-separated data!\")\n",
    "print(\"   GDA's advantage comes when you want to understand the underlying distributions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📈 Confusion Matrix & Detailed Analysis\n",
    "# ============================================================\n",
    "\n",
    "# Fit final LDA model on all training data\n",
    "lda_final = LDA_sklearn()\n",
    "lda_final.fit(X_train, y_train)\n",
    "y_pred = lda_final.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (LDA)')\n",
    "\n",
    "# Classification Report as heatmap\n",
    "report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "report_df = pd.DataFrame(report).iloc[:3, :3].T  # precision, recall, f1 for each class\n",
    "sns.heatmap(report_df, annot=True, fmt='.2f', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Classification Metrics by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📋 Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🧠 LDA for Dimensionality Reduction\n",
    "# ============================================================\n",
    "# A bonus feature of LDA: it can project data to lower dimensions!\n",
    "\n",
    "lda_transform = LDA_sklearn(n_components=2)\n",
    "X_lda = lda_transform.fit_transform(X, y_encoded)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# LDA projection\n",
    "for i, species in enumerate(class_names):\n",
    "    mask = y_encoded == i\n",
    "    axes[0].scatter(X_lda[mask, 0], X_lda[mask, 1], \n",
    "                   label=species, alpha=0.7, s=60, edgecolors='white')\n",
    "\n",
    "axes[0].set_xlabel('LDA Component 1')\n",
    "axes[0].set_ylabel('LDA Component 2')\n",
    "axes[0].set_title('LDA Projection: 4D → 2D')\n",
    "axes[0].legend()\n",
    "\n",
    "# explained variance ratio\n",
    "explained_var = lda_transform.explained_variance_ratio_\n",
    "axes[1].bar(['Component 1', 'Component 2'], explained_var * 100, color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_ylabel('explained Variance (%)')\n",
    "axes[1].set_title('Variance explained by Each LDA Component')\n",
    "for i, v in enumerate(explained_var):\n",
    "    axes[1].text(i, v*100 + 1, f'{v*100:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 LDA's Hidden Power: Dimensionality Reduction!\")\n",
    "print(f\"   • Component 1 explains {explained_var[0]*100:.1f}% of between-class variance\")\n",
    "print(f\"   • Component 2 explains {explained_var[1]*100:.1f}% of between-class variance\")\n",
    "print(\"\\n   Unlike PCA (which maximizes total variance), LDA maximizes CLASS SEPARATION!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Conclusion: Generative vs Discriminative Models\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **GDA is a generative model** – it learns the distribution of each class $P(x|y)$, then uses Bayes' rule\n",
    "2. **LDA** assumes shared covariance → linear boundaries, fewer parameters, lower variance\n",
    "3. **QDA** allows per-class covariance → quadratic boundaries, more flexible, higher variance\n",
    "4. **LDA doubles as dimensionality reduction** – finding projections that maximize class separation\n",
    "\n",
    "### When to Use GDA?\n",
    "\n",
    "| Use GDA When... | Consider Alternatives When... |\n",
    "|-----------------|------------------------------|\n",
    "| Data is roughly Gaussian | Data has complex, non-Gaussian shapes |\n",
    "| You want interpretable parameters | You just need predictions |\n",
    "| Dataset is small | Dataset is large (discriminative models shine) |\n",
    "| Classes have similar/different spreads | Decision boundary is highly non-linear |\n",
    "\n",
    "### The Generative Modeling Perspective\n",
    "\n",
    "The beautiful thing about GDA is that it models **how data is generated**:\n",
    "\n",
    "$$\\text{New penguin} \\leftarrow \\text{Sample species } k \\sim P(y) \\leftarrow \\text{Sample features } x \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "This generative story helps us:\n",
    "- **Detect outliers** (low probability under all classes)\n",
    "- **Generate synthetic data** (sample from the fitted Gaussians)\n",
    "- **Handle missing data** (marginalize over missing features)\n",
    "\n",
    "> *\"Sometimes the best way to classify is to first understand how each class generates its data.\"*\n",
    "\n",
    "---\n",
    "\n",
    "🐧 **Fun Fact**: The Palmer Penguins dataset was collected by Dr. Kristen Gorman at Palmer Station, Antarctica. It's become the modern, more ethically-sourced alternative to the classic Iris dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

